{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "# BERT Fine-Tuning Tutorial with PyTorch\n",
    "\n",
    "By Chris McCormick and Nick Ryan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPgpITmdwvX0"
   },
   "source": [
    "*Revised on March 20, 2020 - Switched to `tokenizer.encode_plus` and added validation loss. See [Revision History](https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=IKzLS9ohzGVu) at the end for details.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJR6t_gCQe_x"
   },
   "source": [
    "In this tutorial I'll show you how to use BERT with the huggingface PyTorch library to quickly and efficiently fine-tune a model to get near state of the art performance in sentence classification. More broadly, I describe the practical application of transfer learning in NLP to create high performance models with minimal effort on a range of NLP tasks.\n",
    "\n",
    "This post is presented in two forms--as a blog post [here](http://mccormickml.com/2019/07/22/BERT-fine-tuning/) and as a Colab Notebook [here](https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX). \n",
    "\n",
    "The content is identical in both, but: \n",
    "* The blog post includes a comments section for discussion. \n",
    "* The Colab Notebook will allow you to run the code and inspect it as you read through.\n",
    "\n",
    "I've also published a video walkthrough of this post on my YouTube channel! [Part 1](https://youtu.be/x66kkDnbzi4) and [Part 2](https://youtu.be/Hnvb9b7a_Ps).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrC9__lXxTJz"
   },
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9MCBOq4xUpr"
   },
   "source": [
    "See \"Table of contents\" in the sidebar to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADkUGTqixRWo"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9vxxTBsuL24"
   },
   "source": [
    "\n",
    "## History\n",
    "\n",
    "2018 was a breakthrough year in NLP. Transfer learning, particularly models like Allen AI's ELMO, OpenAI's Open-GPT, and Google's BERT allowed researchers to smash multiple benchmarks with minimal task-specific fine-tuning and provided the rest of the NLP community with pretrained models that could easily (with less data and less compute time) be fine-tuned and implemented to produce state of the art results. Unfortunately, for many starting out in NLP and even for some experienced practicioners, the theory and practical application of these powerful models is still not well understood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCgvR9INuP5q"
   },
   "source": [
    "\n",
    "## What is BERT?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers), released in late 2018, is the model we will use in this tutorial to provide readers with a better understanding of and practical guidance for using transfer learning models in NLP. BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.\n",
    "\n",
    "This post will explain how you can modify and fine-tune BERT to create a powerful NLP model that quickly gives you state of the art results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaVGdtOkuXUZ"
   },
   "source": [
    "\n",
    "## Advantages of Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5llwu8GBuqMb"
   },
   "source": [
    "\n",
    "In this tutorial, we will use BERT to train a text classifier. Specifically, we will take the pre-trained BERT model, add an untrained layer of neurons on the end, and train the new model for our classification task. Why do this rather than train a train a specific deep learning model (a CNN, BiLSTM, etc.) that is well suited for the specific NLP task you need? \n",
    "\n",
    "1. **Quicker Development**\n",
    "\n",
    "    * First, the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model - it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our classification task. In fact, the authors recommend only 2-4 epochs of training for fine-tuning BERT on a specific NLP task (compared to the hundreds of GPU hours needed to train the original BERT model or a LSTM from scratch!). \n",
    "\n",
    "2. **Less Data**\n",
    "\n",
    "    * In addition and perhaps just as important, because of the pre-trained weights this method allows us to fine-tune our task on a much smaller dataset than would be required in a model that is built from scratch. A major drawback of NLP models built from scratch is that we often need a prohibitively large dataset in order to train our network to reasonable accuracy, meaning a lot of time and energy had to be put into dataset creation. By fine-tuning BERT, we are now able to get away with training a model to good performance on a much smaller amount of training data.\n",
    "\n",
    "3. **Better Results**\n",
    "\n",
    "    * Finally, this simple fine-tuning procedure (typically adding one fully-connected layer on top of BERT and training for a few epochs) was shown to achieve state of the art results with minimal task-specific adjustments for a wide variety of tasks: classification, language inference, semantic similarity, question answering, etc. Rather than implementing custom and sometimes-obscure architetures shown to work well on a specific task, simply fine-tuning BERT is shown to be a better (or at least equal) alternative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEynC5F4u7Nb"
   },
   "source": [
    "\n",
    "### A Shift in NLP\n",
    "\n",
    "This shift to transfer learning parallels the same shift that took place in computer vision a few years ago. Creating a good deep learning network for computer vision tasks can take millions of parameters and be very expensive to train. Researchers discovered that deep networks learn hierarchical feature representations (simple features like edges at the lowest layers with gradually more complex features at higher layers). Rather than training a new network from scratch each time, the lower layers of a trained network with generalized image features could be copied and transfered for use in another network with a different task. It soon became common practice to download a pre-trained deep network and quickly retrain it for the new task or add additional layers on top - vastly preferable to the expensive process of training a network from scratch. For many, the introduction of deep pre-trained language models in 2018 (ELMO, BERT, ULMFIT, Open-GPT, etc.) signals the same shift to transfer learning in NLP that computer vision saw.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Th8bRio6A4"
   },
   "source": [
    "\n",
    "[![BERT eBook Display Ad](https://drive.google.com/uc?export=view&id=1d6L584QYqpREpRIwAZ55Wsq8AUs5qSk1)](https://bit.ly/30JzuBH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX_ZDhicpHkV"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSU7yERLP_66"
   },
   "source": [
    "## 1.1. Using Colab GPU for Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI0iOY8zvZzL"
   },
   "source": [
    "\n",
    "Google Colab offers free GPUs and TPUs! Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time.\n",
    "\n",
    "A GPU can be added by going to the menu and selecting:\n",
    "\n",
    "`Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)`\n",
    "\n",
    "Then run the following cell to confirm that the GPU is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEfSbAA4QHas",
    "outputId": "4ef6084c-1b3b-4f8e-b8dd-92422875a242"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Get the GPU device name.\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# # The device name should look like the following:\n",
    "# if device_name == '/device:GPU:0':\n",
    "#     print('Found GPU at: {}'.format(device_name))\n",
    "# else:\n",
    "#     raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqG7FzRVFEIv"
   },
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYsV4H8fCpZ-",
    "outputId": "2e60467d-6a6b-4898-e2f4-f42884cc6092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1050 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ElsnSNUridI"
   },
   "source": [
    "## 1.2. Installing the Hugging Face Library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_N2UDLevYWn"
   },
   "source": [
    "\n",
    "Next, let's install the [transformers](https://github.com/huggingface/transformers) package from Hugging Face which will give us a pytorch interface for working with BERT. (This library contains interfaces for other pretrained language models like OpenAI's GPT and GPT-2.) We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n",
    "\n",
    "At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task. For example, in this tutorial we will use `BertForSequenceClassification`.\n",
    "\n",
    "The library also includes task-specific classes for token classification, question answering, next sentence prediciton, etc. Using these pre-built classes simplifies the process of modifying BERT for your purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NmMdkZO8R6q",
    "outputId": "5a35e119-1349-4c99-8ec5-37da3fe09833"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxddqmruamSj"
   },
   "source": [
    "The code in this notebook is actually a simplified version of the [run_glue.py](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py) example script from huggingface.\n",
    "\n",
    "`run_glue.py` is a helpful utility which allows you to pick which GLUE benchmark task you want to run on, and which pre-trained model you want to use (you can see the list of possible models [here](https://github.com/huggingface/transformers/blob/e6cff60b4cbc1158fbd6e4a1c3afda8dc224f566/examples/run_glue.py#L69)). It also supports using either the CPU, a single GPU, or multiple GPUs. It even supports using 16-bit precision if you want further speed up.\n",
    "\n",
    "Unfortunately, all of this configurability comes at the cost of *readability*. In this Notebook, we've simplified the code greatly and added plenty of comments to make it clear what's going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "# 2. Loading Train and Val datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9ZKxKc04Btk"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQUy9Tat2EF_"
   },
   "source": [
    "## 2.2. Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeyVCXT31EZQ"
   },
   "source": [
    "We can see from the file names that both `tokenized` and `raw` versions of the data are available. \n",
    "\n",
    "We can't use the pre-tokenized version because, in order to apply the pre-trained BERT, we *must* use the tokenizer provided by the model. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYWzeGSY2xh3"
   },
   "source": [
    "We'll use pandas to parse the \"in-domain\" training set and look at a few of its properties and data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "_UkeC7SG2krJ",
    "outputId": "570f1a76-f163-41e1-b8f5-073bd969f737"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # # Load the dataset into a pandas dataframe.\n",
    "# # df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "# df = pd.read_csv(\"./data/3days/train.csv\")\n",
    "\n",
    "# val_df = pd.read_csv(\"./data/3days/val.csv\")\n",
    "# # Report the number of sentences.\n",
    "# print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "# print(f'Number of validation sentenced: {val_df.shape[0]}')\n",
    "\n",
    "# # Display 10 random rows from the data.\n",
    "# df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      int64\n",
       "ID            float64\n",
       "TEXT           object\n",
       "Label         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfWzpPi92UAH"
   },
   "source": [
    "The two properties we actually care about are the the `sentence` and its `label`, which is referred to as the \"acceptibility judgment\" (0=unacceptable, 1=acceptable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_LpQfzCn9_o"
   },
   "source": [
    "Here are five sentences which are labeled as not grammatically acceptible. Note how much more difficult this task is than something like sentiment analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "blqIvQaQncdJ",
    "outputId": "ea5e6a0b-23b0-42d6-9b05-0e136f6ee5b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30661</th>\n",
       "      <td>sinus arrhythmia left axis deviation - anterio...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46267</th>\n",
       "      <td>1-9 1615 hct recieving lr @ 75 ml/hr presently...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25525</th>\n",
       "      <td>sinus rhythm. normal tracing. no previous trac...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21188</th>\n",
       "      <td>1,000 ml 600 ml blood products: total out: 0 m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37456</th>\n",
       "      <td>tsicu npn 7p-7a s/o- pt sedated over night on ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    TEXT  Label\n",
       "30661  sinus arrhythmia left axis deviation - anterio...    0.0\n",
       "46267  1-9 1615 hct recieving lr @ 75 ml/hr presently...    0.0\n",
       "25525  sinus rhythm. normal tracing. no previous trac...    0.0\n",
       "21188  1,000 ml 600 ml blood products: total out: 0 m...    0.0\n",
       "37456  tsicu npn 7p-7a s/o- pt sedated over night on ...    0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.loc[df.Label == 0].sample(5)[['TEXT', 'Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SMZ5T5Imhlx"
   },
   "source": [
    "\n",
    "\n",
    "Let's extract the sentences and labels of our training set as numpy ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "# # Get the lists of sentences and their labels.\n",
    "# sentences = df.TEXT.values\n",
    "# labels = df.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "# 3. Tokenization & Input Formatting\n",
    "\n",
    "In this section, we'll transform our dataset into the format that BERT can be trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8kEDRvShcU5"
   },
   "source": [
    "## 3.1. BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWOPOyWghJp2"
   },
   "source": [
    "\n",
    "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
    "\n",
    "The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "82ddfcea0e4c4e5a86cf6eca8585be8d",
      "8a256ba4a19e4ec98fe3c3c99fba4daa",
      "8c76faadf2f4415393c6f0a805f0d72b",
      "e0bb735fda99434a90380e7fc664212d",
      "cdb78e75309f4bc09366533331e72431",
      "1058e0b5baa248faa60c1ad146d10bf7",
      "375cc635389c4ddb9bf2aa443df58bae",
      "472198d5b6a748b3a81f9364fd1fa711"
     ]
    },
    "id": "Z474sSC6oe7A",
    "outputId": "4e6d97b6-2d4c-42ca-c201-d2b4a88895b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"./model/pretraining/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFzmtleW6KmJ"
   },
   "source": [
    "Let's apply the tokenizer to one sentence just to see the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLIbudgfh6F0",
    "outputId": "9ca681ff-195f-4960-a0ba-55ded440278e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  have a significant family history of cancer. occupation: drugs: tobacco: alcohol: other: (per omr): the patient is a previous mechanical engineer. he smoked occasionally but quit 35 years ago. he denies any alcohol use. lives alone and is independent. no close relatives in the area. siblings in . review of systems: flowsheet data as of 10:26 pm vital signs hemodynamic monitoring fluid balance 24 hours since am tmax: 6 c (8 tcurrent: 6 c (8 hr: 106 (106 - 121) bpm bp: 129/59(72) {129/58(72) - 130/59(78)} mmhg rr: 19 (19 - 28) insp/min spo2: 95% heart rhythm: st (sinus tachycardia) total in: 382 ml po: tf: ivf: 7 ml blood products: 375 ml total out: 0 ml 210 ml urine: 210 ml ng: stool: drains: balance: 0 ml 172 ml respiratory o2 delivery device: nasal cannula spo2: 95% physical examination vitals: t: 8 bp:130/70 p:113 r: 18 o2: 97on 4l nc general: alert, oriented, no acute distress heent: sclera anicteric, mmm, oropharynx clear neck: supple, jvp 3cm above clavicle, no lad lungs: wheezes anteriorly with rales at bilateral bases cv: tachycardic, irregularly irregular, no murmurs, rubs, gallops abdomen: soft, non-tender, non-distended, bowel sounds present, no rebound tenderness or guarding, no organomegaly gu: no foley ext: warm, well perfused, edema bilaterally to shins. left foot erythematous, warm, slight ttp labs / radiology fluid analysis / other labs: notable for: hct 6 bnp 6751 ck 39 trop 04 mb not done bun/creatinine 39/1 lactate 0 imaging: ct abd/pelvis (wet read - ? error given comment on only lungs . . .) no small bowel obstruction. tree-in- opacities in right middle and bilateral lower lobes, infectious/inflammatory in nature, could represent aspiration. bibasilar atelectasis, but out of proportion to small bilateral pleural effusions, may represent infectious consolidations also. anemia. . echo (): the left atrium is normal in size. the estimated right atrial pressure is 0-5 mmhg. left ventricular wall thicknesses and\n",
      "Tokenized:  ['have', 'a', 'significant', 'family', 'history', 'of', 'cancer', '.', 'occupation', ':', 'drugs', ':', 'tobacco', ':', 'alcohol', ':', 'other', ':', '(', 'per', 'o', '##m', '##r', ')', ':', 'the', 'patient', 'is', 'a', 'previous', 'mechanical', 'engineer', '.', 'he', 'smoked', 'occasionally', 'but', 'quit', '35', 'years', 'ago', '.', 'he', 'denies', 'any', 'alcohol', 'use', '.', 'lives', 'alone', 'and', 'is', 'independent', '.', 'no', 'close', 'relatives', 'in', 'the', 'area', '.', 'siblings', 'in', '.', 'review', 'of', 'systems', ':', 'flows', '##hee', '##t', 'data', 'as', 'of', '10', ':', '26', 'pm', 'vital', 'signs', 'hem', '##ody', '##nam', '##ic', 'monitoring', 'fluid', 'balance', '24', 'hours', 'since', 'am', 't', '##max', ':', '6', 'c', '(', '8', 't', '##current', ':', '6', 'c', '(', '8', 'h', '##r', ':', '106', '(', '106', '-', '121', ')', 'b', '##pm', 'b', '##p', ':', '129', '/', '59', '(', '72', ')', '{', '129', '/', '58', '(', '72', ')', '-', '130', '/', '59', '(', '78', ')', '}', 'mm', '##h', '##g', 'r', '##r', ':', '19', '(', '19', '-', '28', ')', 'ins', '##p', '/', 'min', 's', '##po', '##2', ':', '95', '%', 'heart', 'rhythm', ':', 's', '##t', '(', 'sin', '##us', 'ta', '##chy', '##card', '##ia', ')', 'total', 'in', ':', '38', '##2', 'm', '##l', 'p', '##o', ':', 't', '##f', ':', 'i', '##v', '##f', ':', '7', 'm', '##l', 'blood', 'products', ':', '375', 'm', '##l', 'total', 'out', ':', '0', 'm', '##l', '210', 'm', '##l', 'urine', ':', '210', 'm', '##l', 'ng', ':', 'stool', ':', 'drains', ':', 'balance', ':', '0', 'm', '##l', '172', 'm', '##l', 'respiratory', 'o', '##2', 'delivery', 'device', ':', 'nasal', 'can', '##nu', '##la', 's', '##po', '##2', ':', '95', '%', 'physical', 'examination', 'vital', '##s', ':', 't', ':', '8', 'b', '##p', ':', '130', '/', '70', 'p', ':', '113', 'r', ':', '18', 'o', '##2', ':', '97', '##on', '4', '##l', 'n', '##c', 'general', ':', 'alert', ',', 'oriented', ',', 'no', 'acute', 'distress', 'he', '##ent', ':', 's', '##cle', '##ra', 'an', '##ict', '##eric', ',', 'mm', '##m', ',', 'or', '##op', '##har', '##ynx', 'clear', 'neck', ':', 'su', '##pp', '##le', ',', 'j', '##v', '##p', '3', '##c', '##m', 'above', 'c', '##lav', '##icle', ',', 'no', 'lad', 'lungs', ':', 'w', '##hee', '##zes', 'anterior', '##ly', 'with', 'r', '##ales', 'at', 'bilateral', 'bases', 'c', '##v', ':', 'ta', '##chy', '##card', '##ic', ',', 'irregular', '##ly', 'irregular', ',', 'no', 'murmurs', ',', 'rub', '##s', ',', 'gal', '##lops', 'abdomen', ':', 'soft', ',', 'non', '-', 'tender', ',', 'non', '-', 'di', '##sten', '##ded', ',', 'bow', '##el', 'sounds', 'present', ',', 'no', 're', '##bound', 'tender', '##ness', 'or', 'guarding', ',', 'no', 'organ', '##ome', '##gal', '##y', 'g', '##u', ':', 'no', 'f', '##ole', '##y', 'ex', '##t', ':', 'warm', ',', 'well', 'per', '##fused', ',', 'ed', '##ema', 'bilateral', '##ly', 'to', 's', '##hin', '##s', '.', 'left', 'foot', 'er', '##yt', '##hem', '##ato', '##us', ',', 'warm', ',', 'slight', 't', '##t', '##p', 'labs', '/', 'radio', '##logy', 'fluid', 'analysis', '/', 'other', 'labs', ':', 'notable', 'for', ':', 'h', '##ct', '6', 'b', '##n', '##p', '67', '##51', 'c', '##k', '39', 't', '##rop', '04', 'm', '##b', 'not', 'done', 'b', '##un', '/', 'c', '##rea', '##tin', '##ine', '39', '/', '1', 'la', '##ct', '##ate', '0', 'imaging', ':', 'c', '##t', 'a', '##b', '##d', '/', 'p', '##el', '##vis', '(', 'wet', 'read', '-', '?', 'error', 'given', 'comment', 'on', 'only', 'lungs', '.', '.', '.', ')', 'no', 'small', 'bow', '##el', 'o', '##bs', '##truction', '.', 'tree', '-', 'in', '-', 'op', '##ac', '##ities', 'in', 'right', 'middle', 'and', 'bilateral', 'lower', 'lobes', ',', 'infectious', '/', 'inflammatory', 'in', 'nature', ',', 'could', 'represent', 'as', '##piration', '.', 'bi', '##bas', '##ila', '##r', 'ate', '##lect', '##asis', ',', 'but', 'out', 'of', 'proportion', 'to', 'small', 'bilateral', 'p', '##le', '##ural', 'e', '##ff', '##usions', ',', 'may', 'represent', 'infectious', 'consolidation', '##s', 'also', '.', 'an', '##emia', '.', '.', 'echo', '(', ')', ':', 'the', 'left', 'at', '##rium', 'is', 'normal', 'in', 'size', '.', 'the', 'estimated', 'right', 'at', '##rial', 'pressure', 'is', '0', '-', '5', 'mm', '##h', '##g', '.', 'left', 'vent', '##ric', '##ular', 'wall', 'thickness', '##es', 'and']\n",
      "Token IDs:  [1138, 170, 2418, 1266, 1607, 1104, 4182, 119, 5846, 131, 5557, 131, 10468, 131, 6272, 131, 1168, 131, 113, 1679, 184, 1306, 1197, 114, 131, 1103, 5351, 1110, 170, 2166, 6676, 3806, 119, 1119, 23235, 5411, 1133, 8204, 2588, 1201, 2403, 119, 1119, 26360, 1251, 6272, 1329, 119, 2491, 2041, 1105, 1110, 2457, 119, 1185, 1601, 8908, 1107, 1103, 1298, 119, 9302, 1107, 119, 3189, 1104, 2344, 131, 5611, 19989, 1204, 2233, 1112, 1104, 1275, 131, 1744, 9852, 9301, 5300, 23123, 22320, 12881, 1596, 9437, 8240, 5233, 1572, 2005, 1290, 1821, 189, 22871, 131, 127, 172, 113, 129, 189, 21754, 131, 127, 172, 113, 129, 177, 1197, 131, 9920, 113, 9920, 118, 12794, 114, 171, 9952, 171, 1643, 131, 14949, 120, 4589, 113, 5117, 114, 196, 14949, 120, 4650, 113, 5117, 114, 118, 7029, 120, 4589, 113, 5603, 114, 198, 2608, 1324, 1403, 187, 1197, 131, 1627, 113, 1627, 118, 1743, 114, 22233, 1643, 120, 11241, 188, 5674, 1477, 131, 4573, 110, 1762, 6795, 131, 188, 1204, 113, 11850, 1361, 27629, 8992, 10542, 1465, 114, 1703, 1107, 131, 3383, 1477, 182, 1233, 185, 1186, 131, 189, 2087, 131, 178, 1964, 2087, 131, 128, 182, 1233, 1892, 2982, 131, 19397, 182, 1233, 1703, 1149, 131, 121, 182, 1233, 13075, 182, 1233, 19968, 131, 13075, 182, 1233, 21174, 131, 15631, 131, 20681, 131, 5233, 131, 121, 182, 1233, 19639, 182, 1233, 19192, 184, 1477, 6779, 4442, 131, 21447, 1169, 14787, 1742, 188, 5674, 1477, 131, 4573, 110, 2952, 8179, 9301, 1116, 131, 189, 131, 129, 171, 1643, 131, 7029, 120, 3102, 185, 131, 12206, 187, 131, 1407, 184, 1477, 131, 5311, 1320, 125, 1233, 183, 1665, 1704, 131, 10427, 117, 7779, 117, 1185, 12104, 13632, 1119, 3452, 131, 188, 10536, 1611, 1126, 17882, 26237, 117, 2608, 1306, 117, 1137, 4184, 7111, 22093, 2330, 2455, 131, 28117, 8661, 1513, 117, 179, 1964, 1643, 124, 1665, 1306, 1807, 172, 9516, 26726, 117, 1185, 19122, 8682, 131, 192, 19989, 11846, 16557, 1193, 1114, 187, 19856, 1120, 20557, 7616, 172, 1964, 131, 27629, 8992, 10542, 1596, 117, 12692, 1193, 12692, 117, 1185, 26792, 117, 16259, 1116, 117, 20003, 22101, 14701, 131, 2991, 117, 1664, 118, 8886, 117, 1664, 118, 4267, 15874, 4902, 117, 7125, 1883, 3807, 1675, 117, 1185, 1231, 8346, 8886, 1757, 1137, 18648, 117, 1185, 5677, 6758, 6997, 1183, 176, 1358, 131, 1185, 175, 9016, 1183, 4252, 1204, 131, 3258, 117, 1218, 1679, 21089, 117, 5048, 14494, 20557, 1193, 1106, 188, 8265, 1116, 119, 1286, 2555, 14044, 25669, 15391, 10024, 1361, 117, 3258, 117, 6812, 189, 1204, 1643, 21973, 120, 2070, 6360, 8240, 3622, 120, 1168, 21973, 131, 3385, 1111, 131, 177, 5822, 127, 171, 1179, 1643, 5486, 24050, 172, 1377, 3614, 189, 12736, 5129, 182, 1830, 1136, 1694, 171, 3488, 120, 172, 11811, 6105, 2042, 3614, 120, 122, 2495, 5822, 2193, 121, 14377, 131, 172, 1204, 170, 1830, 1181, 120, 185, 1883, 9356, 113, 4375, 2373, 118, 136, 7353, 1549, 7368, 1113, 1178, 8682, 119, 119, 119, 114, 1185, 1353, 7125, 1883, 184, 4832, 17993, 119, 2780, 118, 1107, 118, 11769, 7409, 4233, 1107, 1268, 2243, 1105, 20557, 2211, 27645, 117, 20342, 120, 22653, 1107, 2731, 117, 1180, 4248, 1112, 22631, 119, 16516, 16531, 8009, 1197, 8756, 18465, 14229, 117, 1133, 1149, 1104, 10807, 1106, 1353, 20557, 185, 1513, 12602, 174, 3101, 27262, 117, 1336, 4248, 20342, 20994, 1116, 1145, 119, 1126, 20504, 119, 119, 16278, 113, 114, 131, 1103, 1286, 1120, 11077, 1110, 2999, 1107, 2060, 119, 1103, 3555, 1268, 1120, 13119, 2997, 1110, 121, 118, 126, 2608, 1324, 1403, 119, 1286, 21828, 4907, 5552, 2095, 15830, 1279, 1105]\n"
     ]
    }
   ],
   "source": [
    "# # Print the original sentence.\n",
    "# print(' Original: ', sentences[0])\n",
    "\n",
    "# # Print the sentence split into tokens.\n",
    "# print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# # Print the sentence mapped to token ids.\n",
    "# print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeNIc4auFUdF"
   },
   "source": [
    "When we actually convert all of our sentences, we'll use the `tokenize.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately. \n",
    "\n",
    "Before we can do that, though, we need to talk about some of BERT's formatting requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viKGCCh8izww"
   },
   "source": [
    "## 3.2. Required Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDcqNlvVhL5W"
   },
   "source": [
    "The above code left out a few required formatting steps that we'll look at here.\n",
    "\n",
    "*Side Note: The input format to BERT seems \"over-specified\" to me... We are required to give it a number of pieces of information which seem redundant, or like they could easily be inferred from the data without us explicity providing it. But it is what it is, and I suspect it will make more sense once I have a deeper understanding of the BERT internals.*\n",
    "\n",
    "We are required to:\n",
    "1. Add special tokens to the start and end of each sentence.\n",
    "2. Pad & truncate all sentences to a single constant length.\n",
    "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6mceWWOjZnw"
   },
   "source": [
    "### Special Tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykk0P9JiKtVe"
   },
   "source": [
    "\n",
    "**`[SEP]`**\n",
    "\n",
    "At the end of every sentence, we need to append the special `[SEP]` token. \n",
    "\n",
    "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). \n",
    "\n",
    "I am not certain yet why the token is still required when we have only single-sentence input, but it is!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86C9objaKu8f"
   },
   "source": [
    "**`[CLS]`**\n",
    "\n",
    "For classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n",
    "\n",
    "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).\n",
    "\n",
    "![Illustration of CLS token purpose](https://drive.google.com/uc?export=view&id=1ck4mvGkznVJfW3hv6GUqcdGepVTOx7HE)\n",
    "\n",
    "On the output of the final (12th) transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier*.\n",
    "\n",
    ">  \"The first token of every sequence is always a special classification token (`[CLS]`). The final hidden state\n",
    "corresponding to this token is used as the aggregate sequence representation for classification\n",
    "tasks.\" (from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "You might think to try some pooling strategy over the final embeddings, but this isn't necessary. Because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector. It's already done the pooling for us!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u51v0kFxeteu"
   },
   "source": [
    "### Sentence Length & Attention Mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPNuwqZVK3T6"
   },
   "source": [
    "The sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n",
    "\n",
    "BERT has two constraints:\n",
    "1. All sentences must be padded or truncated to a single, fixed length.\n",
    "2. The maximum sentence length is 512 tokens.\n",
    "\n",
    "Padding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n",
    "\n",
    "The \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?!). This mask tells the \"Self-Attention\" mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n",
    "\n",
    "The maximum length does impact training and evaluation speed, however. \n",
    "For example, with a Tesla K80:\n",
    "\n",
    "`MAX_LEN = 128  -->  Training epochs take ~5:28 each`\n",
    "\n",
    "`MAX_LEN = 64   -->  Training epochs take ~2:57 each`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6w8elb-58GJ"
   },
   "source": [
    "## 3.3. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U28qy4P-NwQ9"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to perform the real tokenization.\n",
    "\n",
    "The `tokenizer.encode_plus` function combines multiple steps for us:\n",
    "\n",
    "1. Split the sentence into tokens.\n",
    "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
    "3. Map the tokens to their IDs.\n",
    "4. Pad or truncate all sentences to the same length.\n",
    "5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n",
    "\n",
    "The first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKsH2sU0OCQA",
    "outputId": "e363e816-c750-422f-b623-dce428f77502"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from tqdm import trange, tqdm\n",
    "def preprocess_tokenize_embed_data(df_filename):\n",
    "    \n",
    "    '''\n",
    "    Function to use encode_plus to process text data and lables\n",
    "            # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "    input = data file name containing text data/sentences under TEXT column and corresponding labels for classification task\n",
    "    \n",
    "    output = input_ids (encoded sentence id), attention_mask (mask ids for BERT), labels - all converted to tensors\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv(df_filename)\n",
    "    \n",
    "    print('Number of sentences: {:,}\\n'.format(df.shape[0]))    \n",
    "\n",
    "    sentences = df.TEXT.values\n",
    "    labels = df.Label.values\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in tqdm(sentences):\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 512,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    # labels = torch.tensor(labels)\n",
    "    labels = torch.tensor(labels, dtype = torch.long)\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    print('Original: ', sentences[0])\n",
    "    print('Token IDs:', input_ids[0])\n",
    "    \n",
    "    # will teturn a TensorDataset using these input_ids, attention_masks and labels\n",
    "    print(\"returning TensorDataset! \")\n",
    "    return TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M296yz577fV"
   },
   "source": [
    "## 3.4. Training & Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to get the train and validation datasets now - will be ready to train on\n",
    "# this will take a bit of time to do so as the tokenization and encoding is quite a lengthy procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tIWAoWL2RK1p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47793 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "  0%|          | 17/47793 [00:00<04:53, 163.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 47,793\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47793/47793 [04:42<00:00, 169.34it/s]\n",
      "  0%|          | 17/5774 [00:00<00:34, 168.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  have a significant family history of cancer. occupation: drugs: tobacco: alcohol: other: (per omr): the patient is a previous mechanical engineer. he smoked occasionally but quit 35 years ago. he denies any alcohol use. lives alone and is independent. no close relatives in the area. siblings in . review of systems: flowsheet data as of 10:26 pm vital signs hemodynamic monitoring fluid balance 24 hours since am tmax: 6 c (8 tcurrent: 6 c (8 hr: 106 (106 - 121) bpm bp: 129/59(72) {129/58(72) - 130/59(78)} mmhg rr: 19 (19 - 28) insp/min spo2: 95% heart rhythm: st (sinus tachycardia) total in: 382 ml po: tf: ivf: 7 ml blood products: 375 ml total out: 0 ml 210 ml urine: 210 ml ng: stool: drains: balance: 0 ml 172 ml respiratory o2 delivery device: nasal cannula spo2: 95% physical examination vitals: t: 8 bp:130/70 p:113 r: 18 o2: 97on 4l nc general: alert, oriented, no acute distress heent: sclera anicteric, mmm, oropharynx clear neck: supple, jvp 3cm above clavicle, no lad lungs: wheezes anteriorly with rales at bilateral bases cv: tachycardic, irregularly irregular, no murmurs, rubs, gallops abdomen: soft, non-tender, non-distended, bowel sounds present, no rebound tenderness or guarding, no organomegaly gu: no foley ext: warm, well perfused, edema bilaterally to shins. left foot erythematous, warm, slight ttp labs / radiology fluid analysis / other labs: notable for: hct 6 bnp 6751 ck 39 trop 04 mb not done bun/creatinine 39/1 lactate 0 imaging: ct abd/pelvis (wet read - ? error given comment on only lungs . . .) no small bowel obstruction. tree-in- opacities in right middle and bilateral lower lobes, infectious/inflammatory in nature, could represent aspiration. bibasilar atelectasis, but out of proportion to small bilateral pleural effusions, may represent infectious consolidations also. anemia. . echo (): the left atrium is normal in size. the estimated right atrial pressure is 0-5 mmhg. left ventricular wall thicknesses and\n",
      "Token IDs: tensor([  101,  1138,   170,  2418,  1266,  1607,  1104,  4182,   119,  5846,\n",
      "          131,  5557,   131, 10468,   131,  6272,   131,  1168,   131,   113,\n",
      "         1679,   184,  1306,  1197,   114,   131,  1103,  5351,  1110,   170,\n",
      "         2166,  6676,  3806,   119,  1119, 23235,  5411,  1133,  8204,  2588,\n",
      "         1201,  2403,   119,  1119, 26360,  1251,  6272,  1329,   119,  2491,\n",
      "         2041,  1105,  1110,  2457,   119,  1185,  1601,  8908,  1107,  1103,\n",
      "         1298,   119,  9302,  1107,   119,  3189,  1104,  2344,   131,  5611,\n",
      "        19989,  1204,  2233,  1112,  1104,  1275,   131,  1744,  9852,  9301,\n",
      "         5300, 23123, 22320, 12881,  1596,  9437,  8240,  5233,  1572,  2005,\n",
      "         1290,  1821,   189, 22871,   131,   127,   172,   113,   129,   189,\n",
      "        21754,   131,   127,   172,   113,   129,   177,  1197,   131,  9920,\n",
      "          113,  9920,   118, 12794,   114,   171,  9952,   171,  1643,   131,\n",
      "        14949,   120,  4589,   113,  5117,   114,   196, 14949,   120,  4650,\n",
      "          113,  5117,   114,   118,  7029,   120,  4589,   113,  5603,   114,\n",
      "          198,  2608,  1324,  1403,   187,  1197,   131,  1627,   113,  1627,\n",
      "          118,  1743,   114, 22233,  1643,   120, 11241,   188,  5674,  1477,\n",
      "          131,  4573,   110,  1762,  6795,   131,   188,  1204,   113, 11850,\n",
      "         1361, 27629,  8992, 10542,  1465,   114,  1703,  1107,   131,  3383,\n",
      "         1477,   182,  1233,   185,  1186,   131,   189,  2087,   131,   178,\n",
      "         1964,  2087,   131,   128,   182,  1233,  1892,  2982,   131, 19397,\n",
      "          182,  1233,  1703,  1149,   131,   121,   182,  1233, 13075,   182,\n",
      "         1233, 19968,   131, 13075,   182,  1233, 21174,   131, 15631,   131,\n",
      "        20681,   131,  5233,   131,   121,   182,  1233, 19639,   182,  1233,\n",
      "        19192,   184,  1477,  6779,  4442,   131, 21447,  1169, 14787,  1742,\n",
      "          188,  5674,  1477,   131,  4573,   110,  2952,  8179,  9301,  1116,\n",
      "          131,   189,   131,   129,   171,  1643,   131,  7029,   120,  3102,\n",
      "          185,   131, 12206,   187,   131,  1407,   184,  1477,   131,  5311,\n",
      "         1320,   125,  1233,   183,  1665,  1704,   131, 10427,   117,  7779,\n",
      "          117,  1185, 12104, 13632,  1119,  3452,   131,   188, 10536,  1611,\n",
      "         1126, 17882, 26237,   117,  2608,  1306,   117,  1137,  4184,  7111,\n",
      "        22093,  2330,  2455,   131, 28117,  8661,  1513,   117,   179,  1964,\n",
      "         1643,   124,  1665,  1306,  1807,   172,  9516, 26726,   117,  1185,\n",
      "        19122,  8682,   131,   192, 19989, 11846, 16557,  1193,  1114,   187,\n",
      "        19856,  1120, 20557,  7616,   172,  1964,   131, 27629,  8992, 10542,\n",
      "         1596,   117, 12692,  1193, 12692,   117,  1185, 26792,   117, 16259,\n",
      "         1116,   117, 20003, 22101, 14701,   131,  2991,   117,  1664,   118,\n",
      "         8886,   117,  1664,   118,  4267, 15874,  4902,   117,  7125,  1883,\n",
      "         3807,  1675,   117,  1185,  1231,  8346,  8886,  1757,  1137, 18648,\n",
      "          117,  1185,  5677,  6758,  6997,  1183,   176,  1358,   131,  1185,\n",
      "          175,  9016,  1183,  4252,  1204,   131,  3258,   117,  1218,  1679,\n",
      "        21089,   117,  5048, 14494, 20557,  1193,  1106,   188,  8265,  1116,\n",
      "          119,  1286,  2555, 14044, 25669, 15391, 10024,  1361,   117,  3258,\n",
      "          117,  6812,   189,  1204,  1643, 21973,   120,  2070,  6360,  8240,\n",
      "         3622,   120,  1168, 21973,   131,  3385,  1111,   131,   177,  5822,\n",
      "          127,   171,  1179,  1643,  5486, 24050,   172,  1377,  3614,   189,\n",
      "        12736,  5129,   182,  1830,  1136,  1694,   171,  3488,   120,   172,\n",
      "        11811,  6105,  2042,  3614,   120,   122,  2495,  5822,  2193,   121,\n",
      "        14377,   131,   172,  1204,   170,  1830,  1181,   120,   185,  1883,\n",
      "         9356,   113,  4375,  2373,   118,   136,  7353,  1549,  7368,  1113,\n",
      "         1178,  8682,   119,   119,   119,   114,  1185,  1353,  7125,  1883,\n",
      "          184,  4832, 17993,   119,  2780,   118,  1107,   118, 11769,  7409,\n",
      "         4233,   102])\n",
      "returning TensorDataset! \n",
      "Number of sentences: 5,774\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5774/5774 [00:33<00:00, 170.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  10:11 pm chest (pre-op pa & lat) clip # reason: coronary artery disease admitting diagnosis: coronary artery disease medical condition: 77 year old woman with reason for this examination: pre-op final report indication: 77 year old pre-op for cabg. technique: pa and lateral radiographs. comparison: findings: the heart is enlarged. there are bilateral pleural effusions, right greater than left with associated minor atelectatic changes at the lung bases. the pulmonary vasculature appears somewhat prominent with an upper zone redistribution. there is no pneumothorax. the patient has had a prior cabg with unchanged appearance of median sternotomy wires. surgical clips are also seen overlying both right and left hemithoraces. the osseous structures are unremarkable. impression: cardiomegaly with mild congestive heart failure and bilateral pleural effusions. patient/test information: indication: aortic valve disease. coronary artery disease. height: (in) 65 weight (lb): 198 bsa (m2): 97 m2 bp (mm hg): 140/74 hr (bpm): 50 status: inpatient date/time: at 10:53 test: tte (complete) doppler: full doppler and color doppler contrast: none technical quality: adequate interpretation: findings: left atrium: mild la enlargement. right atrium/interatrial septum: mildly dilated ra. left ventricle: mild symmetric lvh. mildly dilated lv cavity. mild regional lv systolic dysfunction. no resting lvot gradient. no lv mass/thrombus. lv wall motion: regional lv wall motion abnormalities include: septal apex - hypo; apex - dyskinetic; right ventricle: normal rv chamber size and free wall motion. aorta: normal aortic root diameter. moderately dilated ascending aorta. aortic valve: severely thickened/deformed aortic valve leaflets. moderate as. mitral valve: mildly thickened mitral valve leaflets. mild mitral annular calcification. mild thickening of mitral valve chordae. mild (1+) mr. tricuspid valve: mild pa systolic hypertension. pericardium: no pericardial effusion. general comments: suboptimal image quality - poor subcostal views. based on aha endocarditis prophylaxis recommendations, the echo findings indicate a moderate risk (prophylaxis recommended). clinical decisions regarding the need for prophylaxis should be based on clinical and echocardiographic data. right pleural\n",
      "Token IDs: tensor([  101,  1275,   131,  1429,  9852,  2229,   113,  3073,   118, 11769,\n",
      "          185,  1161,   111,  2495,  1204,   114, 13500,   108,  2255,   131,\n",
      "         1884, 15789,  1616, 18593,  3653, 19931, 12645,   131,  1884, 15789,\n",
      "         1616, 18593,  3653,  2657,  3879,   131,  5581,  1214,  1385,  1590,\n",
      "         1114,  2255,  1111,  1142,  8179,   131,  3073,   118, 11769,  1509,\n",
      "         2592, 12754,   131,  5581,  1214,  1385,  3073,   118, 11769,  1111,\n",
      "        10347,  1403,   119,  5531,   131,   185,  1161,  1105, 11937,  2070,\n",
      "        21217,   119,  7577,   131,  9505,   131,  1103,  1762,  1110, 12089,\n",
      "          119,  1175,  1132, 20557,   185,  1513, 12602,   174,  3101, 27262,\n",
      "          117,  1268,  3407,  1190,  1286,  1114,  2628,  3137,  8756, 18465,\n",
      "         7698,  2607,  1120,  1103, 13093,  7616,   119,  1103, 26600,   191,\n",
      "         2225, 21608,  5332,  2691,  4742,  3289,  1114,  1126,  3105,  4834,\n",
      "         1894,  1776,  2047, 16442,  1988,   119,  1175,  1110,  1185,   185,\n",
      "         1673,  1818, 12858, 25632,   119,  1103,  5351,  1144,  1125,   170,\n",
      "         2988, 10347,  1403,  1114, 16684,  2468,  1104,  3151, 12172, 12355,\n",
      "         4527, 15923,   119, 13467, 16973,  1132,  1145,  1562, 16201,  1158,\n",
      "         1241,  1268,  1105,  1286, 23123,  7088,  6533,  7723,   119,  1103,\n",
      "          184, 11553,  2285,  4413,  1132,  8362, 16996, 23822,  1895,   119,\n",
      "         8351,   131,  3621,  2660,  3263,  6997,  1183,  1114, 10496, 14255,\n",
      "         7562,  3946,  1762,  4290,  1105, 20557,   185,  1513, 12602,   174,\n",
      "         3101, 27262,   119,  5351,   120,  2774,  1869,   131, 12754,   131,\n",
      "          170, 12148,  1596, 11727,  3653,   119,  1884, 15789,  1616, 18593,\n",
      "         3653,   119,  3976,   131,   113,  1107,   114,  2625,  2841,   113,\n",
      "         5682,   114,   131, 21801,   171,  3202,   113,   182,  1477,   114,\n",
      "          131,  5311,   182,  1477,   171,  1643,   113,  2608,   177,  1403,\n",
      "          114,   131,  8183,   120,  5692,   177,  1197,   113,   171,  9952,\n",
      "          114,   131,  1851,  2781,   131,  1107, 27420,  2236,   120,  1159,\n",
      "          131,  1120,  1275,   131,  4389,  2774,   131,   189,  1566,   113,\n",
      "         2335,   114,  1202,  8661,  2879,   131,  1554,  1202,  8661,  2879,\n",
      "         1105,  2942,  1202,  8661,  2879,  5014,   131,  3839,  4301,  3068,\n",
      "          131, 12373,  7628,   131,  9505,   131,  1286,  1120, 11077,   131,\n",
      "        10496,  2495,  4035,  5815, 20512,   119,  1268,  1120, 11077,   120,\n",
      "         9455,  2980, 13119, 14516,  6451,  1818,   131, 21461,  4267,  6951,\n",
      "          187,  1161,   119,  1286, 21828,  4907,  1513,   131, 10496, 21852,\n",
      "          181,  1964,  1324,   119, 21461,  4267,  6951,   181,  1964, 19421,\n",
      "          119, 10496,  2918,   181,  1964,   188,  6834,  2430,  8031,   173,\n",
      "         6834, 26420,   119,  1185,  8137,   181,  6005,  1204, 19848,   119,\n",
      "         1185,   181,  1964,  3367,   120, 24438, 16071,  7441,   119,   181,\n",
      "         1964,  2095,  4018,   131,  2918,   181,  1964,  2095,  4018, 22832,\n",
      "         4233,  1511,   131, 14516, 21919,  1233, 15764,   118,   177,  1183,\n",
      "         5674,   132, 15764,   118,   173,  6834,  4314,  9265,   132,  1268,\n",
      "        21828,  4907,  1513,   131,  2999,   187,  1964,  5383,  2060,  1105,\n",
      "         1714,  2095,  4018,   119,   170, 12148,  1161,   131,  2999,   170,\n",
      "        12148,  1596,  7261,  6211,   119, 19455,  4267,  6951, 26457,   170,\n",
      "        12148,  1161,   119,   170, 12148,  1596, 11727,   131,  8669,  3528,\n",
      "         4772,   120, 19353, 24211,  1174,   170, 12148,  1596, 11727,  7404,\n",
      "         9585,   119,  8828,  1112,   119, 26410,  4412, 11727,   131, 21461,\n",
      "         3528,  4772, 26410,  4412, 11727,  7404,  9585,   119, 10496, 26410,\n",
      "         4412,  1126, 14787,  5815, 11019,  1233,  6617, 11531,   119, 10496,\n",
      "         3528,  4777,  1104, 26410,  4412, 11727, 15461,  5024,   119, 10496,\n",
      "          113,   122,   116,   114,   182,  1197,   119,   189,  4907,  1361,\n",
      "        25786,   102])\n",
      "returning TensorDataset! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_tokenize_embed_data(\"./data/discharge/train.csv\")\n",
    "val_dataset =  preprocess_tokenize_embed_data(\"./data/discharge/val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47793"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD9i6Z2pG-sN"
   },
   "source": [
    "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n",
      "torch.int64\n",
      "torch.Size([16, 512])\n",
      "torch.int64\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# little test to see correct shapes of train and val datasets\n",
    "\n",
    "# For each batch of training data...\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # Unpack this training batch from our dataloader. \n",
    "    #\n",
    "    # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "    # `to` method.\n",
    "    #\n",
    "    # `batch` contains three pytorch tensors:\n",
    "    #   [0]: input ids \n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels \n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    \n",
    "    print(b_input_ids.dtype)\n",
    "    print(b_input_ids.shape)\n",
    "    print(b_labels.dtype)\n",
    "    print(b_labels.shape)\n",
    "    \n",
    "    # stop after 10 batches.\n",
    "    if step % 10 == 0 and not step == 0:\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bwa6Rts-02-"
   },
   "source": [
    "# 4. Train Our Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xYQ3iLO08SX"
   },
   "source": [
    "Now that our input data is properly formatted, it's time to fine tune the BERT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6TKgyUzPIQc"
   },
   "source": [
    "## 4.1. BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sjzRT1V0zwm"
   },
   "source": [
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. \n",
    "\n",
    "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n",
    "\n",
    "Here is the current list of classes provided for fine-tuning:\n",
    "* BertModel\n",
    "* BertForPreTraining\n",
    "* BertForMaskedLM\n",
    "* BertForNextSentencePrediction\n",
    "* **BertForSequenceClassification** - The one we'll use.\n",
    "* BertForTokenClassification\n",
    "* BertForQuestionAnswering\n",
    "\n",
    "The documentation for these can be found under [here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXYitPoE-cjH"
   },
   "source": [
    "\n",
    "\n",
    "We'll be using [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnQW9E-bBCRt"
   },
   "source": [
    "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "The documentation for `from_pretrained` can be found [here](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), with the additional parameters defined [here](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bf9dfa1ff3e642fbb74c5146d21044c2",
      "1c2b0ede959142fc89bf07a9c88df638",
      "1296a3d754b344a482a03e5af84e805e",
      "6f132d7bb83d41b6847df0d0ec0a1b92",
      "2755b9838bae408ca8cf667ad9d501fc",
      "f8874fec8a404ae89a38fd2ecbb357cf",
      "a7bdbedc75de4f77b45f1389c2ea0abc",
      "978c24b18b594eaf8ca47730a88eefb9",
      "fe254c3bcc08402eb506f0e98f5673a7",
      "cea84f9c3db641acb98314028b305514",
      "23ca9359e6c44232a1346e6f2ab7e48c",
      "d689bc8d488a4dc09c393b4fc9747bcb",
      "6c7dec7b1e804c2195f6e60fb3c1d18e",
      "0fe5b1d0540240a8a8426352c24b2887",
      "4b1e27aff6f04fec8268d951e46b1e63",
      "440da34c72344cb08e4a1ee5de7049ee"
     ]
    },
    "id": "gFsCTp_mporB",
    "outputId": "af690f33-6cd5-4678-bdaf-209f068f70f5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# # linear classification layer on top. \n",
    "# # model = BertForSequenceClassification.from_pretrained(\n",
    "# #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "# #     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "# #                     # You can increase this for multi-class tasks.   \n",
    "# #     output_attentions = False, # Whether the model returns attentions weights.\n",
    "# #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# # )\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"emilyalsentzer/Bio_ClinicalBERT\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "\n",
    "# #set whether we want to freeze all layers other than the new classifier layer\n",
    "# do_freezing = True\n",
    "# if do_freezing:\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'classifier' not in name: # classifier layer\n",
    "#             param.requires_grad = False\n",
    "\n",
    "# # Tell pytorch to run this model on the GPU.\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0140, -0.0392, -0.0104,  ..., -0.0048, -0.0111, -0.0036],\n",
      "        [ 0.0318, -0.0003,  0.0041,  ..., -0.0078,  0.0085, -0.0164]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# or load original clinicalBERT pretraining model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"./model/pretraining/\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "\n",
    "# # Tell pytorch to run this model on the GPU.\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0Jv6c7-HHDW"
   },
   "source": [
    "Just for curiosity's sake, we can browse all of the model's parameters by name here.\n",
    "\n",
    "In the below cell, I've printed out the names and dimensions of the weights for:\n",
    "\n",
    "1. The embedding layer.\n",
    "2. The first of the twelve transformers.\n",
    "3. The output layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PIiVlDYCtSq",
    "outputId": "7430f38d-de86-4488-bb92-6a9b0142b3af"
   },
   "outputs": [],
   "source": [
    "# # Get all of the model's parameters as a list of tuples.\n",
    "# params = list(model.named_parameters())\n",
    "\n",
    "# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "# print('==== Embedding Layer ====\\n')\n",
    "\n",
    "# for p in params[0:5]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "# for p in params[5:21]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "# for p in params[-4:]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRWT-D4U_Pvx"
   },
   "source": [
    "## 4.2. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
    "\n",
    "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
    "\n",
    ">- **Batch size:** 16, 32  \n",
    "- **Learning rate (Adam):** 5e-5, 3e-5, 2e-5  \n",
    "- **Number of epochs:** 2, 3, 4 \n",
    "\n",
    "We chose:\n",
    "* Batch size: 32 (set when creating our DataLoaders)\n",
    "* Learning rate: 2e-5\n",
    "* Epochs: 4 (we'll see that this is probably too many...)\n",
    "\n",
    "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2e-05\n"
     ]
    }
   ],
   "source": [
    "# no_decay = ['bias', 'gamma', 'beta']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "#     ]\n",
    "# optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                      lr=learning_rate,\n",
    "#                      warmup=warmup_proportion,\n",
    "#                      t_total=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                    correct_bias = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-p0upAhhRiIx"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqfmWwUR_Sox"
   },
   "source": [
    "## 4.3. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QXZhFb4LnV5"
   },
   "source": [
    "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n",
    "\n",
    "> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n",
    "\n",
    "**Training:**\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Clear out the gradients calculated in the previous pass. \n",
    "    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
    "- Forward pass (feed input data through the network)\n",
    "- Backward pass (backpropagation)\n",
    "- Tell the network to update parameters with optimizer.step()\n",
    "- Track variables for monitoring progress\n",
    "\n",
    "**Evalution:**\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Forward pass (feed input data through the network)\n",
    "- Compute loss on our validation data and track variables for monitoring progress\n",
    "\n",
    "Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line. \n",
    "\n",
    "> *PyTorch also has some [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) which you may also find helpful.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE5B99H5H2-W"
   },
   "source": [
    "Define a helper function for calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNhRtWPXH9C3"
   },
   "source": [
    "Helper function for formatting elapsed times as `hh:mm:ss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfNIhN19te3N"
   },
   "source": [
    "We're ready to kick off the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell below is for training the classifier with 2 labels/outputsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6J-FYdx6nFE_",
    "outputId": "b2c3e30b-eb5d-4b13-a207-05a48a87ed2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  1,494.    Elapsed: 0:02:49.\n",
      "  Batch 1,000  of  1,494.    Elapsed: 0:05:40.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/181 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:08:28\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/181 [00:00<00:57,  3.11it/s]\u001b[A\n",
      "  1%|          | 2/181 [00:00<00:57,  3.11it/s]\u001b[A\n",
      "  2%|â–         | 3/181 [00:00<00:57,  3.11it/s]\u001b[A\n",
      "  2%|â–         | 4/181 [00:01<00:56,  3.11it/s]\u001b[A\n",
      "  3%|â–Ž         | 5/181 [00:01<00:56,  3.10it/s]\u001b[A\n",
      "  3%|â–Ž         | 6/181 [00:01<00:56,  3.10it/s]\u001b[A\n",
      "  4%|â–         | 7/181 [00:02<00:55,  3.11it/s]\u001b[A\n",
      "  4%|â–         | 8/181 [00:02<00:55,  3.11it/s]\u001b[A\n",
      "  5%|â–         | 9/181 [00:02<00:55,  3.11it/s]\u001b[A\n",
      "  6%|â–Œ         | 10/181 [00:03<00:55,  3.10it/s]\u001b[A\n",
      "  6%|â–Œ         | 11/181 [00:03<00:54,  3.11it/s]\u001b[A\n",
      "  7%|â–‹         | 12/181 [00:03<00:54,  3.11it/s]\u001b[A\n",
      "  7%|â–‹         | 13/181 [00:04<00:54,  3.11it/s]\u001b[A\n",
      "  8%|â–Š         | 14/181 [00:04<00:53,  3.11it/s]\u001b[A\n",
      "  8%|â–Š         | 15/181 [00:04<00:53,  3.11it/s]\u001b[A\n",
      "  9%|â–‰         | 16/181 [00:05<00:53,  3.10it/s]\u001b[A\n",
      "  9%|â–‰         | 17/181 [00:05<00:52,  3.10it/s]\u001b[A\n",
      " 10%|â–‰         | 18/181 [00:05<00:52,  3.11it/s]\u001b[A\n",
      " 10%|â–ˆ         | 19/181 [00:06<00:52,  3.11it/s]\u001b[A\n",
      " 11%|â–ˆ         | 20/181 [00:06<00:51,  3.11it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 21/181 [00:06<00:51,  3.12it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 22/181 [00:07<00:51,  3.11it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 23/181 [00:07<00:50,  3.12it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 24/181 [00:07<00:50,  3.12it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 25/181 [00:08<00:50,  3.11it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 26/181 [00:08<00:49,  3.11it/s]\u001b[A\n",
      " 15%|â–ˆâ–        | 27/181 [00:08<00:49,  3.11it/s]\u001b[A\n",
      " 15%|â–ˆâ–Œ        | 28/181 [00:09<00:49,  3.11it/s]\u001b[A\n",
      " 16%|â–ˆâ–Œ        | 29/181 [00:09<00:48,  3.11it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 30/181 [00:09<00:48,  3.10it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 31/181 [00:09<00:48,  3.10it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 32/181 [00:10<00:48,  3.10it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 33/181 [00:10<00:47,  3.10it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 34/181 [00:10<00:47,  3.11it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 35/181 [00:11<00:46,  3.11it/s]\u001b[A\n",
      " 20%|â–ˆâ–‰        | 36/181 [00:11<00:46,  3.11it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆ        | 37/181 [00:11<00:46,  3.11it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆ        | 38/181 [00:12<00:46,  3.11it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 39/181 [00:12<00:45,  3.10it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 40/181 [00:12<00:45,  3.10it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 41/181 [00:13<00:45,  3.11it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 42/181 [00:13<00:44,  3.11it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 43/181 [00:13<00:44,  3.11it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 44/181 [00:14<00:44,  3.11it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–       | 45/181 [00:14<00:43,  3.11it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 46/181 [00:14<00:43,  3.11it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 47/181 [00:15<00:43,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 48/181 [00:15<00:42,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 49/181 [00:15<00:42,  3.11it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 50/181 [00:16<00:42,  3.11it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 51/181 [00:16<00:41,  3.11it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–Š       | 52/181 [00:16<00:41,  3.11it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–‰       | 53/181 [00:17<00:41,  3.11it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–‰       | 54/181 [00:17<00:40,  3.11it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 55/181 [00:17<00:40,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 56/181 [00:18<00:40,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 57/181 [00:18<00:39,  3.11it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 58/181 [00:18<00:39,  3.11it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 59/181 [00:18<00:39,  3.11it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 60/181 [00:19<00:38,  3.11it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/181 [00:19<00:38,  3.11it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 62/181 [00:19<00:38,  3.11it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 63/181 [00:20<00:37,  3.11it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 64/181 [00:20<00:37,  3.10it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 65/181 [00:20<00:37,  3.11it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 66/181 [00:21<00:37,  3.11it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 67/181 [00:21<00:36,  3.11it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 68/181 [00:21<00:36,  3.11it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 69/181 [00:22<00:36,  3.11it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 70/181 [00:22<00:35,  3.11it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 71/181 [00:22<00:35,  3.11it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 72/181 [00:23<00:34,  3.11it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 73/181 [00:23<00:34,  3.12it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 74/181 [00:23<00:34,  3.12it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/181 [00:24<00:34,  3.12it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 76/181 [00:24<00:33,  3.12it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 77/181 [00:24<00:33,  3.11it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 78/181 [00:25<00:33,  3.11it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 79/181 [00:25<00:32,  3.11it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/181 [00:25<00:32,  3.11it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/181 [00:26<00:32,  3.11it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 82/181 [00:26<00:31,  3.11it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 83/181 [00:26<00:31,  3.11it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/181 [00:27<00:31,  3.11it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 85/181 [00:27<00:30,  3.11it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 86/181 [00:27<00:30,  3.11it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 87/181 [00:27<00:30,  3.11it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 88/181 [00:28<00:29,  3.11it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 89/181 [00:28<00:29,  3.11it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 90/181 [00:28<00:29,  3.11it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 91/181 [00:29<00:28,  3.11it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 92/181 [00:29<00:28,  3.11it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 93/181 [00:29<00:28,  3.11it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/181 [00:30<00:27,  3.11it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 95/181 [00:30<00:27,  3.11it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 96/181 [00:30<00:27,  3.11it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 97/181 [00:31<00:26,  3.11it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/181 [00:31<00:26,  3.11it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/181 [00:31<00:26,  3.11it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 100/181 [00:32<00:26,  3.11it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 101/181 [00:32<00:25,  3.11it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 102/181 [00:32<00:25,  3.11it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 103/181 [00:33<00:25,  3.11it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 104/181 [00:33<00:24,  3.11it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 105/181 [00:33<00:24,  3.11it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 106/181 [00:34<00:24,  3.10it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 107/181 [00:34<00:23,  3.10it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 108/181 [00:34<00:23,  3.11it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 109/181 [00:35<00:23,  3.11it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 110/181 [00:35<00:22,  3.11it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 111/181 [00:35<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 112/181 [00:36<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 113/181 [00:36<00:21,  3.10it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 114/181 [00:36<00:21,  3.11it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 115/181 [00:36<00:21,  3.11it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/181 [00:37<00:20,  3.11it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/181 [00:37<00:20,  3.11it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 118/181 [00:37<00:20,  3.10it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 119/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 120/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 121/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 122/181 [00:39<00:18,  3.11it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 123/181 [00:39<00:18,  3.11it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 124/181 [00:39<00:18,  3.11it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 125/181 [00:40<00:18,  3.11it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 126/181 [00:40<00:17,  3.11it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 127/181 [00:40<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 128/181 [00:41<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 129/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 130/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 131/181 [00:42<00:16,  3.10it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 132/181 [00:42<00:15,  3.11it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 133/181 [00:42<00:15,  3.11it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/181 [00:43<00:15,  3.10it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/181 [00:43<00:14,  3.11it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 136/181 [00:43<00:14,  3.11it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 137/181 [00:44<00:14,  3.11it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 138/181 [00:44<00:13,  3.11it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 139/181 [00:44<00:13,  3.11it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 140/181 [00:45<00:13,  3.11it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 141/181 [00:45<00:12,  3.11it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/181 [00:45<00:12,  3.10it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 143/181 [00:46<00:12,  3.10it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 144/181 [00:46<00:11,  3.10it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 145/181 [00:46<00:11,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 146/181 [00:46<00:11,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 147/181 [00:47<00:10,  3.10it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 148/181 [00:47<00:10,  3.11it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 149/181 [00:47<00:10,  3.11it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 150/181 [00:48<00:09,  3.11it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 151/181 [00:48<00:09,  3.10it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/181 [00:48<00:09,  3.10it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/181 [00:49<00:09,  3.10it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 154/181 [00:49<00:08,  3.10it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 155/181 [00:49<00:08,  3.10it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 156/181 [00:50<00:08,  3.11it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 157/181 [00:50<00:07,  3.10it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 158/181 [00:50<00:07,  3.11it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 159/181 [00:51<00:07,  3.11it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 160/181 [00:51<00:06,  3.11it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 161/181 [00:51<00:06,  3.10it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 162/181 [00:52<00:06,  3.10it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 163/181 [00:52<00:05,  3.10it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 164/181 [00:52<00:05,  3.10it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 165/181 [00:53<00:05,  3.10it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 166/181 [00:53<00:04,  3.10it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 167/181 [00:53<00:04,  3.10it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 168/181 [00:54<00:04,  3.10it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 169/181 [00:54<00:03,  3.11it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 170/181 [00:54<00:03,  3.10it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/181 [00:55<00:03,  3.10it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 172/181 [00:55<00:02,  3.10it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 173/181 [00:55<00:02,  3.10it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 174/181 [00:55<00:02,  3.10it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 175/181 [00:56<00:01,  3.10it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 176/181 [00:56<00:01,  3.11it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 177/181 [00:56<00:01,  3.11it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 178/181 [00:57<00:00,  3.11it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 179/181 [00:57<00:00,  3.11it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 180/181 [00:57<00:00,  3.11it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:58<00:00,  3.12it/s]\u001b[A\n",
      "Epoch:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [09:26<28:18, 566.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.54\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  1,494.    Elapsed: 0:02:51.\n",
      "  Batch 1,000  of  1,494.    Elapsed: 0:05:42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/181 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:08:30\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/181 [00:00<00:57,  3.12it/s]\u001b[A\n",
      "  1%|          | 2/181 [00:00<00:57,  3.11it/s]\u001b[A\n",
      "  2%|â–         | 3/181 [00:00<00:57,  3.10it/s]\u001b[A\n",
      "  2%|â–         | 4/181 [00:01<00:56,  3.11it/s]\u001b[A\n",
      "  3%|â–Ž         | 5/181 [00:01<00:56,  3.10it/s]\u001b[A\n",
      "  3%|â–Ž         | 6/181 [00:01<00:56,  3.10it/s]\u001b[A\n",
      "  4%|â–         | 7/181 [00:02<00:56,  3.10it/s]\u001b[A\n",
      "  4%|â–         | 8/181 [00:02<00:55,  3.10it/s]\u001b[A\n",
      "  5%|â–         | 9/181 [00:02<00:55,  3.10it/s]\u001b[A\n",
      "  6%|â–Œ         | 10/181 [00:03<00:55,  3.10it/s]\u001b[A\n",
      "  6%|â–Œ         | 11/181 [00:03<00:54,  3.10it/s]\u001b[A\n",
      "  7%|â–‹         | 12/181 [00:03<00:54,  3.10it/s]\u001b[A\n",
      "  7%|â–‹         | 13/181 [00:04<00:54,  3.10it/s]\u001b[A\n",
      "  8%|â–Š         | 14/181 [00:04<00:53,  3.10it/s]\u001b[A\n",
      "  8%|â–Š         | 15/181 [00:04<00:53,  3.11it/s]\u001b[A\n",
      "  9%|â–‰         | 16/181 [00:05<00:53,  3.10it/s]\u001b[A\n",
      "  9%|â–‰         | 17/181 [00:05<00:52,  3.10it/s]\u001b[A\n",
      " 10%|â–‰         | 18/181 [00:05<00:52,  3.10it/s]\u001b[A\n",
      " 10%|â–ˆ         | 19/181 [00:06<00:52,  3.10it/s]\u001b[A\n",
      " 11%|â–ˆ         | 20/181 [00:06<00:51,  3.10it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 21/181 [00:06<00:51,  3.10it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 22/181 [00:07<00:51,  3.10it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 23/181 [00:07<00:50,  3.10it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 24/181 [00:07<00:50,  3.11it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 25/181 [00:08<00:50,  3.11it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 26/181 [00:08<00:49,  3.11it/s]\u001b[A\n",
      " 15%|â–ˆâ–        | 27/181 [00:08<00:49,  3.11it/s]\u001b[A\n",
      " 15%|â–ˆâ–Œ        | 28/181 [00:09<00:49,  3.11it/s]\u001b[A\n",
      " 16%|â–ˆâ–Œ        | 29/181 [00:09<00:48,  3.11it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 30/181 [00:09<00:48,  3.11it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 31/181 [00:09<00:48,  3.11it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 32/181 [00:10<00:48,  3.10it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 33/181 [00:10<00:47,  3.10it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 34/181 [00:10<00:47,  3.11it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 35/181 [00:11<00:46,  3.11it/s]\u001b[A\n",
      " 20%|â–ˆâ–‰        | 36/181 [00:11<00:46,  3.10it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆ        | 37/181 [00:11<00:46,  3.10it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆ        | 38/181 [00:12<00:46,  3.10it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 39/181 [00:12<00:45,  3.10it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 40/181 [00:12<00:45,  3.11it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 41/181 [00:13<00:45,  3.11it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 42/181 [00:13<00:44,  3.10it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 43/181 [00:13<00:44,  3.10it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 44/181 [00:14<00:44,  3.10it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–       | 45/181 [00:14<00:43,  3.11it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 46/181 [00:14<00:43,  3.11it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 47/181 [00:15<00:43,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 48/181 [00:15<00:42,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 49/181 [00:15<00:42,  3.10it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 50/181 [00:16<00:42,  3.10it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 51/181 [00:16<00:41,  3.10it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–Š       | 52/181 [00:16<00:41,  3.10it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–‰       | 53/181 [00:17<00:41,  3.11it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–‰       | 54/181 [00:17<00:40,  3.11it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 55/181 [00:17<00:40,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 56/181 [00:18<00:40,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 57/181 [00:18<00:39,  3.11it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 58/181 [00:18<00:39,  3.10it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 59/181 [00:19<00:39,  3.11it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 60/181 [00:19<00:38,  3.10it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/181 [00:19<00:38,  3.10it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 62/181 [00:19<00:38,  3.10it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 63/181 [00:20<00:38,  3.10it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 64/181 [00:20<00:37,  3.10it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 65/181 [00:20<00:37,  3.10it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 66/181 [00:21<00:37,  3.10it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 67/181 [00:21<00:36,  3.10it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 68/181 [00:21<00:36,  3.10it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 69/181 [00:22<00:36,  3.10it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 70/181 [00:22<00:35,  3.10it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 71/181 [00:22<00:35,  3.10it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 72/181 [00:23<00:35,  3.10it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 73/181 [00:23<00:34,  3.10it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 74/181 [00:23<00:34,  3.10it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/181 [00:24<00:34,  3.10it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 76/181 [00:24<00:33,  3.10it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 77/181 [00:24<00:33,  3.09it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 78/181 [00:25<00:33,  3.09it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 79/181 [00:25<00:32,  3.10it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/181 [00:25<00:32,  3.10it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/181 [00:26<00:32,  3.10it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 82/181 [00:26<00:31,  3.10it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 83/181 [00:26<00:31,  3.10it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/181 [00:27<00:31,  3.10it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 85/181 [00:27<00:31,  3.10it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 86/181 [00:27<00:30,  3.10it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 87/181 [00:28<00:30,  3.10it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 88/181 [00:28<00:30,  3.09it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 89/181 [00:28<00:29,  3.10it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 90/181 [00:29<00:29,  3.10it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 91/181 [00:29<00:29,  3.10it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 92/181 [00:29<00:28,  3.10it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 93/181 [00:29<00:28,  3.10it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/181 [00:30<00:28,  3.10it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 95/181 [00:30<00:27,  3.10it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 96/181 [00:30<00:27,  3.10it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 97/181 [00:31<00:27,  3.10it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/181 [00:31<00:26,  3.10it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/181 [00:31<00:26,  3.10it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 100/181 [00:32<00:26,  3.10it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 101/181 [00:32<00:25,  3.10it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 102/181 [00:32<00:25,  3.10it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 103/181 [00:33<00:25,  3.10it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 104/181 [00:33<00:24,  3.10it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 105/181 [00:33<00:24,  3.10it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 106/181 [00:34<00:24,  3.10it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 107/181 [00:34<00:23,  3.10it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 108/181 [00:34<00:23,  3.10it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 109/181 [00:35<00:23,  3.10it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 110/181 [00:35<00:22,  3.10it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 111/181 [00:35<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 112/181 [00:36<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 113/181 [00:36<00:21,  3.10it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 114/181 [00:36<00:21,  3.10it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 115/181 [00:37<00:21,  3.11it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/181 [00:37<00:20,  3.11it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/181 [00:37<00:20,  3.11it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 118/181 [00:38<00:20,  3.11it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 119/181 [00:38<00:19,  3.11it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 120/181 [00:38<00:19,  3.11it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 121/181 [00:39<00:19,  3.11it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 122/181 [00:39<00:19,  3.10it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 123/181 [00:39<00:18,  3.11it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 124/181 [00:39<00:18,  3.11it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 125/181 [00:40<00:18,  3.11it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 126/181 [00:40<00:17,  3.11it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 127/181 [00:40<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 128/181 [00:41<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 129/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 130/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 131/181 [00:42<00:16,  3.10it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 132/181 [00:42<00:15,  3.10it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 133/181 [00:42<00:15,  3.10it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/181 [00:43<00:15,  3.10it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/181 [00:43<00:14,  3.11it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 136/181 [00:43<00:14,  3.11it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 137/181 [00:44<00:14,  3.11it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 138/181 [00:44<00:13,  3.11it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 139/181 [00:44<00:13,  3.11it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 140/181 [00:45<00:13,  3.11it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 141/181 [00:45<00:12,  3.11it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/181 [00:45<00:12,  3.11it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 143/181 [00:46<00:12,  3.10it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 144/181 [00:46<00:11,  3.09it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 145/181 [00:46<00:11,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 146/181 [00:47<00:11,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 147/181 [00:47<00:10,  3.10it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 148/181 [00:47<00:10,  3.11it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 149/181 [00:48<00:10,  3.11it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 150/181 [00:48<00:09,  3.11it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 151/181 [00:48<00:09,  3.11it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/181 [00:48<00:09,  3.11it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/181 [00:49<00:09,  3.11it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 154/181 [00:49<00:08,  3.11it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 155/181 [00:49<00:08,  3.11it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 156/181 [00:50<00:08,  3.11it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 157/181 [00:50<00:07,  3.11it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 158/181 [00:50<00:07,  3.11it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 159/181 [00:51<00:07,  3.11it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 160/181 [00:51<00:06,  3.11it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 161/181 [00:51<00:06,  3.11it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 162/181 [00:52<00:06,  3.10it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 163/181 [00:52<00:05,  3.10it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 164/181 [00:52<00:05,  3.11it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 165/181 [00:53<00:05,  3.10it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 166/181 [00:53<00:04,  3.09it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 167/181 [00:53<00:04,  3.09it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 168/181 [00:54<00:04,  3.10it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 169/181 [00:54<00:03,  3.10it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 170/181 [00:54<00:03,  3.11it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/181 [00:55<00:03,  3.10it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 172/181 [00:55<00:02,  3.10it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 173/181 [00:55<00:02,  3.11it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 174/181 [00:56<00:02,  3.11it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 175/181 [00:56<00:01,  3.10it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 176/181 [00:56<00:01,  3.10it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 177/181 [00:57<00:01,  3.10it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 178/181 [00:57<00:00,  3.10it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 179/181 [00:57<00:00,  3.10it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 180/181 [00:58<00:00,  3.10it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:58<00:00,  3.11it/s]\u001b[A\n",
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [18:54<18:53, 566.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.54\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  1,494.    Elapsed: 0:02:51.\n",
      "  Batch 1,000  of  1,494.    Elapsed: 0:05:42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/181 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:08:31\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/181 [00:00<00:57,  3.13it/s]\u001b[A\n",
      "  1%|          | 2/181 [00:00<00:57,  3.12it/s]\u001b[A\n",
      "  2%|â–         | 3/181 [00:00<00:57,  3.11it/s]\u001b[A\n",
      "  2%|â–         | 4/181 [00:01<00:56,  3.11it/s]\u001b[A\n",
      "  3%|â–Ž         | 5/181 [00:01<00:56,  3.10it/s]\u001b[A\n",
      "  3%|â–Ž         | 6/181 [00:01<00:56,  3.10it/s]\u001b[A\n",
      "  4%|â–         | 7/181 [00:02<00:56,  3.11it/s]\u001b[A\n",
      "  4%|â–         | 8/181 [00:02<00:55,  3.10it/s]\u001b[A\n",
      "  5%|â–         | 9/181 [00:02<00:55,  3.10it/s]\u001b[A\n",
      "  6%|â–Œ         | 10/181 [00:03<00:55,  3.10it/s]\u001b[A\n",
      "  6%|â–Œ         | 11/181 [00:03<00:54,  3.10it/s]\u001b[A\n",
      "  7%|â–‹         | 12/181 [00:03<00:54,  3.10it/s]\u001b[A\n",
      "  7%|â–‹         | 13/181 [00:04<00:54,  3.10it/s]\u001b[A\n",
      "  8%|â–Š         | 14/181 [00:04<00:53,  3.10it/s]\u001b[A\n",
      "  8%|â–Š         | 15/181 [00:04<00:53,  3.10it/s]\u001b[A\n",
      "  9%|â–‰         | 16/181 [00:05<00:53,  3.10it/s]\u001b[A\n",
      "  9%|â–‰         | 17/181 [00:05<00:52,  3.10it/s]\u001b[A\n",
      " 10%|â–‰         | 18/181 [00:05<00:52,  3.10it/s]\u001b[A\n",
      " 10%|â–ˆ         | 19/181 [00:06<00:52,  3.10it/s]\u001b[A\n",
      " 11%|â–ˆ         | 20/181 [00:06<00:51,  3.11it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 21/181 [00:06<00:51,  3.11it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 22/181 [00:07<00:51,  3.11it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 23/181 [00:07<00:50,  3.11it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 24/181 [00:07<00:50,  3.11it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 25/181 [00:08<00:50,  3.11it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 26/181 [00:08<00:49,  3.11it/s]\u001b[A\n",
      " 15%|â–ˆâ–        | 27/181 [00:08<00:49,  3.10it/s]\u001b[A\n",
      " 15%|â–ˆâ–Œ        | 28/181 [00:09<00:49,  3.10it/s]\u001b[A\n",
      " 16%|â–ˆâ–Œ        | 29/181 [00:09<00:49,  3.10it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 30/181 [00:09<00:48,  3.10it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 31/181 [00:09<00:48,  3.10it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 32/181 [00:10<00:48,  3.10it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 33/181 [00:10<00:47,  3.10it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 34/181 [00:10<00:47,  3.11it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 35/181 [00:11<00:46,  3.11it/s]\u001b[A\n",
      " 20%|â–ˆâ–‰        | 36/181 [00:11<00:46,  3.11it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆ        | 37/181 [00:11<00:46,  3.11it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆ        | 38/181 [00:12<00:46,  3.11it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 39/181 [00:12<00:45,  3.10it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 40/181 [00:12<00:45,  3.10it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 41/181 [00:13<00:45,  3.11it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 42/181 [00:13<00:44,  3.11it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 43/181 [00:13<00:44,  3.11it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 44/181 [00:14<00:44,  3.10it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–       | 45/181 [00:14<00:43,  3.10it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 46/181 [00:14<00:43,  3.10it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 47/181 [00:15<00:43,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 48/181 [00:15<00:42,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 49/181 [00:15<00:42,  3.11it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 50/181 [00:16<00:42,  3.11it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 51/181 [00:16<00:41,  3.11it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–Š       | 52/181 [00:16<00:41,  3.11it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–‰       | 53/181 [00:17<00:41,  3.11it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–‰       | 54/181 [00:17<00:40,  3.10it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 55/181 [00:17<00:40,  3.10it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 56/181 [00:18<00:40,  3.10it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 57/181 [00:18<00:40,  3.10it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 58/181 [00:18<00:39,  3.10it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 59/181 [00:19<00:39,  3.10it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 60/181 [00:19<00:38,  3.10it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/181 [00:19<00:38,  3.11it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 62/181 [00:19<00:38,  3.10it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 63/181 [00:20<00:37,  3.11it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 64/181 [00:20<00:37,  3.11it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 65/181 [00:20<00:37,  3.10it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 66/181 [00:21<00:37,  3.10it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 67/181 [00:21<00:36,  3.10it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 68/181 [00:21<00:36,  3.10it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 69/181 [00:22<00:36,  3.10it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 70/181 [00:22<00:35,  3.11it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 71/181 [00:22<00:35,  3.11it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 72/181 [00:23<00:35,  3.10it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 73/181 [00:23<00:34,  3.10it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 74/181 [00:23<00:34,  3.10it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/181 [00:24<00:34,  3.10it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 76/181 [00:24<00:33,  3.10it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 77/181 [00:24<00:33,  3.10it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 78/181 [00:25<00:33,  3.10it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 79/181 [00:25<00:32,  3.10it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/181 [00:25<00:32,  3.10it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/181 [00:26<00:32,  3.10it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 82/181 [00:26<00:31,  3.10it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 83/181 [00:26<00:31,  3.10it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/181 [00:27<00:31,  3.10it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 85/181 [00:27<00:31,  3.10it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 86/181 [00:27<00:30,  3.09it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 87/181 [00:28<00:30,  3.10it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 88/181 [00:28<00:30,  3.10it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 89/181 [00:28<00:29,  3.10it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 90/181 [00:29<00:29,  3.09it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 91/181 [00:29<00:29,  3.10it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 92/181 [00:29<00:28,  3.10it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 93/181 [00:29<00:28,  3.10it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/181 [00:30<00:28,  3.10it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 95/181 [00:30<00:27,  3.11it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 96/181 [00:30<00:27,  3.11it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 97/181 [00:31<00:27,  3.10it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/181 [00:31<00:26,  3.10it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/181 [00:31<00:26,  3.10it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 100/181 [00:32<00:26,  3.10it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 101/181 [00:32<00:25,  3.10it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 102/181 [00:32<00:25,  3.09it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 103/181 [00:33<00:25,  3.10it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 104/181 [00:33<00:24,  3.10it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 105/181 [00:33<00:24,  3.10it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 106/181 [00:34<00:24,  3.10it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 107/181 [00:34<00:23,  3.10it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 108/181 [00:34<00:23,  3.11it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 109/181 [00:35<00:23,  3.11it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 110/181 [00:35<00:22,  3.11it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 111/181 [00:35<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 112/181 [00:36<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 113/181 [00:36<00:21,  3.11it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 114/181 [00:36<00:21,  3.11it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 115/181 [00:37<00:21,  3.11it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/181 [00:37<00:20,  3.10it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/181 [00:37<00:20,  3.10it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 118/181 [00:38<00:20,  3.10it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 119/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 120/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 121/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 122/181 [00:39<00:19,  3.10it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 123/181 [00:39<00:18,  3.10it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 124/181 [00:39<00:18,  3.10it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 125/181 [00:40<00:18,  3.10it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 126/181 [00:40<00:17,  3.10it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 127/181 [00:40<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 128/181 [00:41<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 129/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 130/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 131/181 [00:42<00:16,  3.10it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 132/181 [00:42<00:15,  3.10it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 133/181 [00:42<00:15,  3.10it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/181 [00:43<00:15,  3.10it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/181 [00:43<00:14,  3.09it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 136/181 [00:43<00:14,  3.09it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 137/181 [00:44<00:14,  3.09it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 138/181 [00:44<00:13,  3.09it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 139/181 [00:44<00:13,  3.10it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 140/181 [00:45<00:13,  3.10it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 141/181 [00:45<00:12,  3.10it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/181 [00:45<00:12,  3.09it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 143/181 [00:46<00:12,  3.09it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 144/181 [00:46<00:11,  3.10it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 145/181 [00:46<00:11,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 146/181 [00:47<00:11,  3.09it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 147/181 [00:47<00:10,  3.09it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 148/181 [00:47<00:10,  3.09it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 149/181 [00:48<00:10,  3.10it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 150/181 [00:48<00:10,  3.10it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 151/181 [00:48<00:09,  3.10it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/181 [00:49<00:09,  3.10it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/181 [00:49<00:09,  3.10it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 154/181 [00:49<00:08,  3.10it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 155/181 [00:49<00:08,  3.10it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 156/181 [00:50<00:08,  3.11it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 157/181 [00:50<00:07,  3.11it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 158/181 [00:50<00:07,  3.10it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 159/181 [00:51<00:07,  3.10it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 160/181 [00:51<00:06,  3.10it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 161/181 [00:51<00:06,  3.10it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 162/181 [00:52<00:06,  3.09it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 163/181 [00:52<00:05,  3.10it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 164/181 [00:52<00:05,  3.10it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 165/181 [00:53<00:05,  3.10it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 166/181 [00:53<00:04,  3.10it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 167/181 [00:53<00:04,  3.11it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 168/181 [00:54<00:04,  3.11it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 169/181 [00:54<00:03,  3.11it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 170/181 [00:54<00:03,  3.10it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/181 [00:55<00:03,  3.10it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 172/181 [00:55<00:02,  3.10it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 173/181 [00:55<00:02,  3.10it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 174/181 [00:56<00:02,  3.10it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 175/181 [00:56<00:01,  3.10it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 176/181 [00:56<00:01,  3.10it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 177/181 [00:57<00:01,  3.10it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 178/181 [00:57<00:00,  3.11it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 179/181 [00:57<00:00,  3.11it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 180/181 [00:58<00:00,  3.11it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:58<00:00,  3.11it/s]\u001b[A\n",
      "Epoch:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [28:23<09:27, 567.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.54\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  1,494.    Elapsed: 0:02:51.\n",
      "  Batch 1,000  of  1,494.    Elapsed: 0:05:42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/181 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:08:30\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/181 [00:00<00:57,  3.13it/s]\u001b[A\n",
      "  1%|          | 2/181 [00:00<00:57,  3.13it/s]\u001b[A\n",
      "  2%|â–         | 3/181 [00:00<00:57,  3.12it/s]\u001b[A\n",
      "  2%|â–         | 4/181 [00:01<00:56,  3.12it/s]\u001b[A\n",
      "  3%|â–Ž         | 5/181 [00:01<00:56,  3.11it/s]\u001b[A\n",
      "  3%|â–Ž         | 6/181 [00:01<00:56,  3.11it/s]\u001b[A\n",
      "  4%|â–         | 7/181 [00:02<00:56,  3.11it/s]\u001b[A\n",
      "  4%|â–         | 8/181 [00:02<00:55,  3.10it/s]\u001b[A\n",
      "  5%|â–         | 9/181 [00:02<00:55,  3.10it/s]\u001b[A\n",
      "  6%|â–Œ         | 10/181 [00:03<00:55,  3.11it/s]\u001b[A\n",
      "  6%|â–Œ         | 11/181 [00:03<00:54,  3.11it/s]\u001b[A\n",
      "  7%|â–‹         | 12/181 [00:03<00:54,  3.11it/s]\u001b[A\n",
      "  7%|â–‹         | 13/181 [00:04<00:54,  3.11it/s]\u001b[A\n",
      "  8%|â–Š         | 14/181 [00:04<00:53,  3.11it/s]\u001b[A\n",
      "  8%|â–Š         | 15/181 [00:04<00:53,  3.11it/s]\u001b[A\n",
      "  9%|â–‰         | 16/181 [00:05<00:53,  3.11it/s]\u001b[A\n",
      "  9%|â–‰         | 17/181 [00:05<00:52,  3.11it/s]\u001b[A\n",
      " 10%|â–‰         | 18/181 [00:05<00:52,  3.11it/s]\u001b[A\n",
      " 10%|â–ˆ         | 19/181 [00:06<00:52,  3.11it/s]\u001b[A\n",
      " 11%|â–ˆ         | 20/181 [00:06<00:51,  3.11it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 21/181 [00:06<00:51,  3.11it/s]\u001b[A\n",
      " 12%|â–ˆâ–        | 22/181 [00:07<00:51,  3.11it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 23/181 [00:07<00:50,  3.11it/s]\u001b[A\n",
      " 13%|â–ˆâ–Ž        | 24/181 [00:07<00:50,  3.11it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 25/181 [00:08<00:50,  3.11it/s]\u001b[A\n",
      " 14%|â–ˆâ–        | 26/181 [00:08<00:49,  3.11it/s]\u001b[A\n",
      " 15%|â–ˆâ–        | 27/181 [00:08<00:49,  3.11it/s]\u001b[A\n",
      " 15%|â–ˆâ–Œ        | 28/181 [00:09<00:49,  3.11it/s]\u001b[A\n",
      " 16%|â–ˆâ–Œ        | 29/181 [00:09<00:48,  3.11it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 30/181 [00:09<00:48,  3.11it/s]\u001b[A\n",
      " 17%|â–ˆâ–‹        | 31/181 [00:09<00:48,  3.11it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 32/181 [00:10<00:47,  3.11it/s]\u001b[A\n",
      " 18%|â–ˆâ–Š        | 33/181 [00:10<00:47,  3.11it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 34/181 [00:10<00:47,  3.11it/s]\u001b[A\n",
      " 19%|â–ˆâ–‰        | 35/181 [00:11<00:47,  3.10it/s]\u001b[A\n",
      " 20%|â–ˆâ–‰        | 36/181 [00:11<00:46,  3.10it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆ        | 37/181 [00:11<00:46,  3.10it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆ        | 38/181 [00:12<00:46,  3.11it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 39/181 [00:12<00:45,  3.10it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–       | 40/181 [00:12<00:45,  3.10it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 41/181 [00:13<00:45,  3.10it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 42/181 [00:13<00:44,  3.10it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 43/181 [00:13<00:44,  3.10it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–       | 44/181 [00:14<00:44,  3.10it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–       | 45/181 [00:14<00:43,  3.10it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 46/181 [00:14<00:43,  3.10it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 47/181 [00:15<00:43,  3.10it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 48/181 [00:15<00:42,  3.10it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–‹       | 49/181 [00:15<00:42,  3.10it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 50/181 [00:16<00:42,  3.10it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 51/181 [00:16<00:41,  3.10it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–Š       | 52/181 [00:16<00:41,  3.10it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–‰       | 53/181 [00:17<00:41,  3.10it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–‰       | 54/181 [00:17<00:40,  3.11it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 55/181 [00:17<00:40,  3.10it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 56/181 [00:18<00:40,  3.09it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 57/181 [00:18<00:40,  3.09it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 58/181 [00:18<00:39,  3.09it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 59/181 [00:19<00:39,  3.09it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 60/181 [00:19<00:39,  3.09it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/181 [00:19<00:38,  3.09it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 62/181 [00:19<00:38,  3.09it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 63/181 [00:20<00:38,  3.10it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 64/181 [00:20<00:37,  3.10it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 65/181 [00:20<00:37,  3.10it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 66/181 [00:21<00:37,  3.10it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 67/181 [00:21<00:36,  3.10it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 68/181 [00:21<00:36,  3.10it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 69/181 [00:22<00:36,  3.10it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 70/181 [00:22<00:35,  3.10it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 71/181 [00:22<00:35,  3.10it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 72/181 [00:23<00:35,  3.10it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 73/181 [00:23<00:34,  3.10it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 74/181 [00:23<00:34,  3.10it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/181 [00:24<00:34,  3.10it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 76/181 [00:24<00:33,  3.10it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 77/181 [00:24<00:33,  3.10it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 78/181 [00:25<00:33,  3.10it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 79/181 [00:25<00:32,  3.10it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/181 [00:25<00:32,  3.10it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/181 [00:26<00:32,  3.10it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 82/181 [00:26<00:31,  3.10it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 83/181 [00:26<00:31,  3.09it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/181 [00:27<00:31,  3.10it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 85/181 [00:27<00:30,  3.11it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 86/181 [00:27<00:30,  3.11it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 87/181 [00:28<00:30,  3.10it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 88/181 [00:28<00:29,  3.10it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 89/181 [00:28<00:29,  3.10it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 90/181 [00:29<00:29,  3.10it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 91/181 [00:29<00:29,  3.10it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 92/181 [00:29<00:28,  3.10it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 93/181 [00:29<00:28,  3.10it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/181 [00:30<00:28,  3.10it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 95/181 [00:30<00:27,  3.10it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 96/181 [00:30<00:27,  3.11it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 97/181 [00:31<00:27,  3.10it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/181 [00:31<00:26,  3.10it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/181 [00:31<00:26,  3.11it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 100/181 [00:32<00:26,  3.11it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 101/181 [00:32<00:25,  3.10it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 102/181 [00:32<00:25,  3.10it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 103/181 [00:33<00:25,  3.10it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 104/181 [00:33<00:24,  3.09it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 105/181 [00:33<00:24,  3.09it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 106/181 [00:34<00:24,  3.09it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 107/181 [00:34<00:23,  3.10it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 108/181 [00:34<00:23,  3.10it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 109/181 [00:35<00:23,  3.10it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 110/181 [00:35<00:22,  3.10it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 111/181 [00:35<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 112/181 [00:36<00:22,  3.10it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 113/181 [00:36<00:21,  3.10it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 114/181 [00:36<00:21,  3.10it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 115/181 [00:37<00:21,  3.10it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/181 [00:37<00:20,  3.10it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/181 [00:37<00:20,  3.10it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 118/181 [00:38<00:20,  3.10it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 119/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 120/181 [00:38<00:19,  3.10it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 121/181 [00:39<00:19,  3.11it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 122/181 [00:39<00:18,  3.11it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 123/181 [00:39<00:18,  3.10it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 124/181 [00:39<00:18,  3.10it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 125/181 [00:40<00:18,  3.10it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 126/181 [00:40<00:17,  3.10it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 127/181 [00:40<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 128/181 [00:41<00:17,  3.10it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 129/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 130/181 [00:41<00:16,  3.10it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 131/181 [00:42<00:16,  3.11it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 132/181 [00:42<00:15,  3.11it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 133/181 [00:42<00:15,  3.10it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/181 [00:43<00:15,  3.10it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/181 [00:43<00:14,  3.09it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 136/181 [00:43<00:14,  3.09it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 137/181 [00:44<00:14,  3.09it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 138/181 [00:44<00:13,  3.10it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 139/181 [00:44<00:13,  3.10it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 140/181 [00:45<00:13,  3.10it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 141/181 [00:45<00:12,  3.10it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/181 [00:45<00:12,  3.11it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 143/181 [00:46<00:12,  3.11it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 144/181 [00:46<00:11,  3.11it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 145/181 [00:46<00:11,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 146/181 [00:47<00:11,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 147/181 [00:47<00:10,  3.10it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 148/181 [00:47<00:10,  3.11it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 149/181 [00:48<00:10,  3.11it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 150/181 [00:48<00:09,  3.11it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 151/181 [00:48<00:09,  3.10it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/181 [00:49<00:09,  3.10it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/181 [00:49<00:09,  3.10it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 154/181 [00:49<00:08,  3.10it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 155/181 [00:49<00:08,  3.11it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 156/181 [00:50<00:08,  3.11it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 157/181 [00:50<00:07,  3.11it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 158/181 [00:50<00:07,  3.11it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 159/181 [00:51<00:07,  3.11it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 160/181 [00:51<00:06,  3.11it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 161/181 [00:51<00:06,  3.10it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 162/181 [00:52<00:06,  3.10it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 163/181 [00:52<00:05,  3.10it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 164/181 [00:52<00:05,  3.10it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 165/181 [00:53<00:05,  3.11it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 166/181 [00:53<00:04,  3.11it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 167/181 [00:53<00:04,  3.11it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 168/181 [00:54<00:04,  3.11it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 169/181 [00:54<00:03,  3.11it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 170/181 [00:54<00:03,  3.11it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/181 [00:55<00:03,  3.11it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 172/181 [00:55<00:02,  3.11it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 173/181 [00:55<00:02,  3.11it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 174/181 [00:56<00:02,  3.11it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 175/181 [00:56<00:01,  3.11it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 176/181 [00:56<00:01,  3.11it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 177/181 [00:57<00:01,  3.11it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 178/181 [00:57<00:00,  3.11it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 179/181 [00:57<00:00,  3.11it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 180/181 [00:58<00:00,  3.11it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:58<00:00,  3.11it/s]\u001b[A\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [37:51<00:00, 567.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.54\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:58\n",
      "Training complete!\n",
      "Total training took 0:37:52 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "# for epoch_i in range(0, epochs):\n",
    "for epoch_i in trange(epochs, desc=\"Epoch\"):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(b_input_ids.dtype)\n",
    "#         print(b_labels.dtype)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "#         loss, logits = model(b_input_ids, \n",
    "#                          token_type_ids=None, \n",
    "#                          attention_mask=b_input_mask, \n",
    "#                          labels=b_labels)\n",
    "        \n",
    "#         print(loss)\n",
    "#         print(logits)\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         print(b_input_ids.dtype)\n",
    "#         print(b_labels.dtype)\n",
    "        \n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "#             loss, logits = model(b_input_ids, \n",
    "#                      token_type_ids=None, \n",
    "#                      attention_mask=b_input_mask, \n",
    "#                      labels=b_labels)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQTvJ1vRP7u4"
   },
   "source": [
    "Let's view the summary of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "6O_NbXFGMukX",
    "outputId": "a9e51eda-5eae-4800-87d5-8d016ff25bb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0:08:28</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0:08:30</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0:08:31</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0:08:30</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.70         0.69           0.54       0:08:28         0:00:58\n",
       "2               0.69         0.69           0.54       0:08:30         0:00:58\n",
       "3               0.69         0.69           0.54       0:08:31         0:00:58\n",
       "4               0.69         0.69           0.54       0:08:30         0:00:58"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "\n",
    "output_dir = './w_freeze_results_updated_clinBERTpretrained_3day_200421/'\n",
    "\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_stats.to_csv(f'{output_dir}training_stats.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-G03mmwH3aI"
   },
   "source": [
    "Notice that, while the the training loss is going down with each epoch, the validation loss is increasing! This suggests that we are training our model too long, and it's over-fitting on the training data. \n",
    "\n",
    "(For reference, we are using 7,695 training samples and 856 validation samples).\n",
    "\n",
    "Validation Loss is a more precise measure than accuracy, because with accuracy we don't care about the exact output value, but just which side of a threshold it falls on. \n",
    "\n",
    "If we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "68xreA9JAmG5",
    "outputId": "70b8500d-7efc-4c99-de1f-05e8795e6298"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "# plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(f'{output_dir}train_val_loss_figure.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "# 5. Performance On Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DosV94BYIYxg"
   },
   "source": [
    "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_score(df, score, output_dir):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    # score\n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max) + df_sort.groupby(['ID'])['pred_score'].agg(sum) / 2) / (\n",
    "                1 + df_sort.groupby(['ID'])['pred_score'].agg(len) / 2)\n",
    "    x = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "    df_out = pd.DataFrame({'logits': temp.values, 'ID': x})\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(x, temp.values)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='Val (area = {:.3f})'.format(auc_score))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.savefig(f'{output_dir}auroc_BioClinicalbert_discharge.png')\n",
    "    plt.show()\n",
    "    \n",
    "#     string = 'auroc_clinicalbert_' + args.readmission_mode + '.png'\n",
    "#     plt.savefig(os.path.join(args.output_dir, string))\n",
    "\n",
    "    return fpr, tpr, df_out\n",
    "\n",
    "\n",
    "def pr_curve_plot(y, y_score, output_dir):\n",
    "    precision, recall, _ = precision_recall_curve(y, y_score)\n",
    "    area = auc(recall, precision)\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: AUC={0:0.2f}'.format(\n",
    "        area))\n",
    "    \n",
    "    plt.savefig(f'{output_dir}auprc_BioClinicalbert_discharge.png')\n",
    "    plt.show()\n",
    "#     string = 'auprc_clinicalbert_' + args.readmission_mode + '.png'\n",
    "\n",
    "#     plt.savefig(os.path.join(args.output_dir, string))\n",
    "\n",
    "\n",
    "def vote_pr_curve(df, score, output_dir):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    # score\n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max) + df_sort.groupby(['ID'])['pred_score'].agg(sum) / 2) / (\n",
    "                1 + df_sort.groupby(['ID'])['pred_score'].agg(len) / 2)\n",
    "    y = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "\n",
    "    precision, recall, thres = precision_recall_curve(y, temp)\n",
    "    pr_thres = pd.DataFrame(data=list(zip(precision, recall, thres)), columns=['prec', 'recall', 'thres'])\n",
    "    vote_df = pd.DataFrame(data=list(zip(temp, y)), columns=['score', 'label'])\n",
    "\n",
    "    pr_curve_plot(y, temp, output_dir)\n",
    "\n",
    "    temp = pr_thres[pr_thres.prec > 0.799999].reset_index()\n",
    "\n",
    "    rp80 = 0\n",
    "    if temp.size == 0:\n",
    "        print('Test Sample too small or RP80=0')\n",
    "    else:\n",
    "        rp80 = temp.iloc[0].recall\n",
    "        print('Recall at Precision of 80 is {}', rp80)\n",
    "\n",
    "    return rp80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg42jJqqM68F"
   },
   "source": [
    "### 5.1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWe0_JW21MyV"
   },
   "source": [
    "\n",
    "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAN0LZBOOPVh",
    "outputId": "7385ca3f-72d5-45f0-bbfe-5056c2f62c4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3063 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\ntaylor\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "  0%|â–                                                                                | 6/3063 [00:00<00:58, 52.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3,063\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3063/3063 [00:51<00:00, 59.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  date of birth: sex: f service: medicine allergies: haldol attending: chief complaint: delta ms, lethargy, ?sepsis . major surgical or invasive procedure: none history of present illness: hx obtained per ed notes and sister . hpi: 35f with disease who presented today from daycare after her healthcare providers noted that she was lethargic. they were initially unable to obtain a blood pressure. the patient was noted to have a very rapid heart rate. vitals were finally obtained and were as follows: bp 70/50 (baseline sbps 80-90), hr 113, o2 sat 99% on 3l nc. . the patient was transferred to where she was noted to have a temp of 4, hr 200 and sbp 80s. ekg was noteworthy for a wide complex tachycardia. the patient received adenosine 6mg and then 12mg with no improvement. she was cardioverted into sinus rhythm. her d-dimer was elevated at 3590, lactate was 5 and trop t 39 in the setting of renal insufficiency. a ct-a was negative for a pe. the patient was transferred to the micu for further mgmt. . past medical history: disease anemia nonverbal at baseline . social history: meds: tylenol ensure . sochx: patient lives at home with sister and brother. she also goes to daycare. she is non-verbal at baseline. . family history: father who passed away of dz physical exam: t 7, hr 65-68, bp 91-97/61-63, r 14-21, o2 sat 100%2l gen: thin appearing female lying in fetal position in nad heent: mm dry, op clear heart: nl rate, s1s2, no gmr lungs: cta b/l abd: flat, soft, nt, nd, +bs, negative guardin, negative rebound tenderness ext: wwp, +dp b/l neuro: unable to assess . pertinent results: ct-a impression: no evidence of pulmonary embolism. poorly defined opacities within the lungs bilaterally, possibly representing combination of atelectasis and consolidation. air bronchograms in the right middle lobe suggests possible infection. . cxr impression: left lower lobe process suggesting\n",
      "Token IDs: tensor([  101,  2236,  1104,  3485,   131,  2673,   131,   175,  1555,   131,\n",
      "         5182,  1155,  1200, 19310,   131,  5871, 25791,  1233,  6546,   131,\n",
      "         2705, 12522,   131, 20811,   182,  1116,   117,  1519,  7111,  4873,\n",
      "          117,   136, 14516, 17990,   119,  1558, 13467,  1137, 19849,  7791,\n",
      "          131,  3839,  1607,  1104,  1675,  6946,   131,   177,  1775,  3836,\n",
      "         1679,  5048,  3697,  1105,  2104,   119,  6857,  1182,   131,  2588,\n",
      "         2087,  1114,  3653,  1150,  2756,  2052,  1121,  1285, 23340,  1170,\n",
      "         1123, 12520, 12263,  2382,  1115,  1131,  1108,  1519,  7111, 11007,\n",
      "          119,  1152,  1127,  2786,  3372,  1106,  6268,   170,  1892,  2997,\n",
      "          119,  1103,  5351,  1108,  2382,  1106,  1138,   170,  1304,  6099,\n",
      "         1762,  2603,   119,  9301,  1116,  1127,  1921,  3836,  1105,  1127,\n",
      "         1112,  3226,   131,   171,  1643,  3102,   120,  1851,   113,  2259,\n",
      "         2568,   188,  1830,  3491,  2908,   118,  3078,   114,   117,   177,\n",
      "         1197, 12206,   117,   184,  1477,  2068,  4850,   110,  1113,   124,\n",
      "         1233,   183,  1665,   119,   119,  1103,  5351,  1108,  3175,  1106,\n",
      "         1187,  1131,  1108,  2382,  1106,  1138,   170, 21359,  8223,  1104,\n",
      "          125,   117,   177,  1197,  2363,  1105,   188,  1830,  1643, 17008,\n",
      "          119,   174,  1377,  1403,  1108, 22076,  1111,   170,  2043,  2703,\n",
      "        27629,  8992, 10542,  1465,   119,  1103,  5351,  1460,  8050, 26601,\n",
      "        10606,  1162,   127,  1306,  1403,  1105,  1173,  1367,  1306,  1403,\n",
      "         1114,  1185,  8331,   119,  1131,  1108,  3621,  2660, 17534,  1154,\n",
      "        11850,  1361,  6795,   119,  1123,   173,   118, 12563,  1200,  1108,\n",
      "         8208,  1120,  2588, 21500,   117,  2495,  5822,  2193,  1108,   126,\n",
      "         1105,   189, 12736,   189,  3614,  1107,  1103,  3545,  1104,  1231,\n",
      "         7050, 22233,  9435, 23864,   119,   170,   172,  1204,   118,   170,\n",
      "         1108,  4366,  1111,   170,   185,  1162,   119,  1103,  5351,  1108,\n",
      "         3175,  1106,  1103,  1940, 10182,  1111,  1748, 17713,  1306,  1204,\n",
      "          119,   119,  1763,  2657,  1607,   131,  3653,  1126, 20504,  1664,\n",
      "         4121,  7767,  1120,  2259,  2568,   119,  1934,  1607,   131,  1143,\n",
      "         3680,   131,   189, 12415,  2728,  1233,  4989,   119,  1177,  1732,\n",
      "         1775,   131,  5351,  2491,  1120,  1313,  1114,  2104,  1105,  1711,\n",
      "          119,  1131,  1145,  2947,  1106,  1285, 23340,   119,  1131,  1110,\n",
      "         1664,   118, 14093,  1120,  2259,  2568,   119,   119,  1266,  1607,\n",
      "          131,  1401,  1150,  2085,  1283,  1104,   173,  1584,  2952, 12211,\n",
      "          131,   189,   128,   117,   177,  1197,  2625,   118,  5599,   117,\n",
      "          171,  1643,  5539,   118,  5311,   120,  5391,   118,  5519,   117,\n",
      "          187,  1489,   118,  1626,   117,   184,  1477,  2068,  1620,   110,\n",
      "          123,  1233,   176,  1424,   131,  4240,  5452,  2130,  4009,  1107,\n",
      "          175, 21470,  1700,  1107,  9468,  1181,  1119,  3452,   131,  2608,\n",
      "         3712,   117, 11769,  2330,  1762,   131,   183,  1233,  2603,   117,\n",
      "          188,  1475,  1116,  1477,   117,  1185,   176,  1306,  1197,  8682,\n",
      "          131,   172,  1777,   171,   120,   181,   170,  1830,  1181,   131,\n",
      "         3596,   117,  2991,   117,   183,  1204,   117,   183,  1181,   117,\n",
      "          116,   171,  1116,   117,  4366,  3542,  1394,   117,  4366,  1231,\n",
      "         8346,  8886,  1757,  4252,  1204,   131,   192,  2246,  1643,   117,\n",
      "          116,   173,  1643,   171,   120,   181, 24928, 11955,   131,  3372,\n",
      "         1106, 15187,   119,  1679, 24123,  2686,   131,   172,  1204,   118,\n",
      "          170,  8351,   131,  1185,  2554,  1104, 26600,  9712, 15792,  1863,\n",
      "          119,  9874,  3393, 11769,  7409,  4233,  1439,  1103,  8682, 20557,\n",
      "         1193,   117,  3566,  4311,  4612,  1104,  8756, 18465, 14229,  1105,\n",
      "        20994,   119,  1586,  9304,  1320,  8401, 12139,  1116,  1107,  1103,\n",
      "         1268,   102])\n",
      "returning TensorDataset! \n",
      "3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = preprocess_tokenize_embed_data(\"./data/discharge/test.csv\")\n",
    "\n",
    "print(len(test_dataset))\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16lctEOyNFik"
   },
   "source": [
    "## 5.2. Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cells below are for 2 label classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhR99IISNMg9"
   },
   "source": [
    "\n",
    "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     './model/updated_BioClinicalBERT_3day_190421/', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hba10sXR7Xi6",
    "outputId": "e35f0a6e-72c5-4bd0-9c4b-dcec9ef5059d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/96 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [04:06<00:00,  2.57s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRU533/8fdXGwIESIjFGLPaYMALNMFrvDuJjbs4ad3GS9amdd0mbX9tfz3J6a/N3jZNT9skJ04cN3bc/JLGv7ZxUyf1kj12HTs22BgbO7YBYxCLQAiE0IK27++P7x3PIEZXI6GRRuLzOmcOM/feufPMBe5nnue5z3PN3RERERlI2VgXQERESpuCQkREUikoREQklYJCRERSKShERCSVgkJERFIpKGRQZnaLmX2vgO3uMLO/Go0yjQYz225mb06ef8zMvj7WZRIZCwqKcS45mXWY2REzazSzr5pZzUh+hrt/w93fWsB2t7n7J0fyszPMzM2sLfmeu8zsH82svBifNdGZ2T1m1mNmp+ZZ/ql+yxYnx74iZ9nNZrY++bvYY2YPmtklwyjHn5jZXjNrMbO7zWxSyrblZvYpM9ttZq1m9oyZ1SbrLFm3K9nXT8zsrKGWRwamoJgYftXda4A3AOcBf9l/g9z/6OPY6uR7Xg68A/jtMS7PiBqNvyMzmwr8BtAC3DKM9/8p8Fngb4C5wELgi8D1Q9zPNcCHgauBxcBS4OMpb/k4cDFwETAdeBfQmaz7TeLfwqXATOBx4P8OpTySTkExgbj7LuBB4Gx4/Vf4B8zsFeCVZNmvmNlGMztkZj8zs3Mz7zezBWZ2n5ntN7MDZvaFZPl7zex/kudmZv9kZvuSX2+bzCzzecf8IjWz3zWzLWbWbGb35/6CTcp2m5m9YmYHzex2M7MCv+cW4DFgTc7+hvO9TjezHyXLmszsG5lfqUNlZtcnn3/YzLaa2bXJ8tebr5LXrzdh5fxaf7+Z7QB+ZGYPmdkH++37WTP79eT5CjP7fnJMXzKz3xpiUX8DOAR8AnjPEL/jjOR9H3D3+9y9zd273f077v7nQyzHe4C73H2zux8EPgm8d4DPrQP+F/C77v6ah+fdPRMUS4D/cfdt7t4LfB1YNcTySAoFxQRiZguA64Bncha/DbgAWGVmbwDuBn4PqAe+DNxvZpOSZpzvAq8Rv/DmA/fm+Zi3ApcBy4Fa4pf9gTxluQr4W+C3gHnJfvvv71eIGtDqZLtrCvyeK4hfj1uS18P9XpaU8VRgJbAA+FghZehXnvOBrwF/ThyTy4DtQ9jF5cnnXwP8K3BTzr5XAYuA/05qA99PtpmTbPfFTDNL0iS0aZDPeg/wTeIYrEiOXaEuAqqB/xxog6QMh1IeC5NNzwKezXnrs8BcM6vPs9tzgB7ghqSp6mUz+0DO+nuBM8xsuZlVJt/xoSF8LxmMu+sxjh/ECekI8SvxNaIZYHKyzoGrcrb9EvDJfu9/iThRXQTsByryfMZ7iV9sAFcBLwMXAmX9trsH+FTy/C7gMznraoBuYHFO2S7JWf9vwIdTvqcDh4G25Pk3gUkn8r3yfMbbgGf6Hds3J88/Bnx9gPd9GfinlL+fN+e8fn0/RHA5sDRn/bTkOy5KXv81cHfy/B3Ao3k++6MF/ltZCPQBa5LXDwOfy/f3l7MsU8YKoqlq7wj9u90KXJvzujL5nMV5tr05WXcXMBk4N/k7fUuyvgr4XLJND/AqsGS0/g+eDA/VKCaGt7l7rbsvcvc/cPeOnHU7c54vAv4s9xce8Sv61OTP19y9J+2D3P1HwBeA24FGM7vTzKbn2fRUIrgy7ztC1Dzm52yzN+d5OxEmmNlmi47SI2Z2ac42b0i2eQdRS5p6It/LzOaY2b0WnaCHiSaLWWnffwALiBPfcL3+d+TurcB/Azcmi24EvpE8XwRc0O973gKcUuDnvAt40d03Jq+/Adyc/AqHOMlW9ntPJREufcTf3ywbmb6UI0RfQ0bmeWuebTP/nj/h7h3uvomoRVyXLP8oUTNdQNR4Pk40400ZgXIKano6GeROD7wT+OskVDKPKe7+zWTdwkJOAu7+eXd/I9F8sJxoculvN3FiA17vRK0HdhWw/7PcvSZ5PNpvnbv7vxEdlh85we/1t8TxOdfdpwPvJJqjhmoncPoA69qA3BNWvpN6/ymcvwncZGYXEb+gf5zzOT/t9z1r3P33Cyznu4GlSfPNXuAfiWBcl6zfQdQgci0Bdrp7H3HMO4maV14Wl1IfSXlkmp42E02OGauBRnc/rhkTyDSnDTTV9Wrg/7l7g7v3uPs9QB3qpxgxCoqTyz8Dt5nZBRammtkvm9k04ElgD/DpZHm1mb2p/w7M7Lzk/ZXESbAT6M3zWf8KvM/M1lhc9vg3wM/dffsIfZdPA7ea2Skn8L2mkTTbmdl88gdeIe4ivuvVZlZmZvOTfhSAjcCNZlZpZmuBGwrY3wNEyH6COAH2Jcu/Cyw3s3cl+6tM/j5WDrbDJHROB84nLgJYQ1z08K9kO7W/Bfyymb3V4nLUU4kr6O4FcPcWIpxvN7O3mdmUpAzrzOwzyTbfyAn5fI8dyWd9DXi/ma1KOqv/kmj6Oo67bwUeBf5P0u+0kqhVfjfZ5CngN81sbnL830XUhLYMdlykQGPd9qXHiT3o1wbeb50DZ/Rbdi3xH+sQcQL9d2Basm4h8G2iiaEJ+Hyy/L1k+yiuJn7hHUm2+QZQk6y7h5w2buA2okmmmfhPfdpAZev/3gK/y4PAP5zA9zoL2JB8l43AnwEN+Y4tKX0Uyfq3J8ellThBXZMsXwr8PPmM/wY+z/F9FPn6he5K1p3Xb/mZyX72J9/nR2T7HG4BNg9QvjuAb+VZfj5wFJiZvP7V5Ji0EE2Hf0/S55XznluA9cQPhb1JeS4exr/dPwUaib6nr5L0OeX83f5Fzuv5RAf1EWAb8Hs566qJptA9yb6eJqf/Q48Tf1hyoEVERPJS05OIiKRSUIiISCoFhYiIpFJQiIhIqnE3UdysWbN88eLFY10MEZFxZcOGDU3uPns47x13QbF48WLWr18/1sUQERlXzOy1wbfKT01PIiKSSkEhIiKpFBQiIpJKQSEiIqkUFCIikkpBISIiqYoWFGZ2t8V9lZ8fYL2Z2ect7qm8aYi3ZBQRkVFSzBrFPcTUzwNZByxLHrcSt7MUEZESU7SgcPdHiPsQDOR64GsengBqzWzeYPs9cgQ0M7qIyOgZyz6K+Rx7P+cGjr2f8uvM7FYzW29m65uaDtKcFj8iIjKixjIo8t2bOG9dwd3vdPe17r52+vQ61ShEREbRWAZFA7Ag5/VpwO4xKouIiAxgLIPifuDdydVPFwIt7r5nDMsjIiJ5FG32WDP7JnAFMMvMGoCPApUA7n4H8ABwHXEj+nbgfcUqi4iIDF/RgsLdbxpkvQMfKNbni4jIyNDIbBERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSXVSB0VHBxw8CF1dY10SEZHSVbRpxktdZyf84hfQ0AB1dXDJJWNdIhGR0nRS1ih6e2HTJti9G7Ztg5dfHusSiYiUrpOuRnH0aIREczM0NsK0acPbT3t7NF3V1MCkSSNbRhGRUnJSBUVXF+zYESGxezcsWAAtLXD48ND2s3Mn7NkTj9pauPzy4pRXRKQUnFRB0dwczUyNjTB3LsycGUGRcfQolJVBZWX+97e1wQsvQGtr7KelBWbNUlCIyMR20gTF3r1xcm9thfnzob7+2PVNTdG5ffQoXHBBNCll9PZGp3dDQ2y3Z0/URmbNOjZoREQmopMiKDo7o3lpxw6oqop+CbPs+vZ2eOkleO012LULli7NBkVnZwTM/v0REAArV8b6V1+NfooXXoj9nnHG6H83EZFiOymCYteuqC10dcGSJcd2Pnd0xJ9bt0J3N1TkHJFME9OuXdGnMW8enHJKdpvMezdsiCCaPRtmzBid7yQiMlom/OWxR45EraCpCRYtOv4KJfeoUbjH+qqqWH7oUITLtm3Rp7FsWTRZVfSL1sOHo7mqpwf6+kbnO4mIjKYJX6N49dXsOIn+TU4ZR4/C2WdHDQIiXPbtg+3bo9awYgVMmXL8+5Yti31OmRJNVyIiE9GEDoqenmhOOnQoTurl5cdvc9ZZUZPItX17hEd3d/Q7TJ6cf/+VldGpfeDAiBddRKRkTOigePbZaHKqqICpUwfeLtNx3d4eAZG5kmnRooFDQkTkZDGh+yja2mJw3Ny5hW3f1xdh0dwMp52Wv7lJRORkM+5rFAcPxsl9xoxjxz40NsafU6bE1UiFmD07ahBLlw5/ag8RkYlm3AfF1q2wZUsExbp1sayzMzqXM+MmClVRAVdeOfQyHD0af+7YEeUom9D1NBE52Yz7U1pPT9Qe9u49dnlXV5z4Fy4sfhna2uJKqccei/EWIiITybgPCjj2klf3bGd0efnA8zaNpFNOicA6eDCulOovM7r75ZfzrxcRKWUTIihyHT4cA+WamvJfDlsM06bBZZfB9OnHLu/ri+aoZ56JaT4efjgG8ImIjCdFDQozu9bMXjKzLWb24TzrZ5jZd8zsWTPbbGbvO9HPdM9OG37KKSe6t+Hr6IjLc7dsgVdeiSupurqi5iEiMp4ULSjMrBy4HVgHrAJuMrNV/Tb7APCCu68GrgD+wcyG0P08sKlTx+6GQgcOwObNUZt48cWocZx+um5wJCLjUzGvejof2OLu2wDM7F7geuCFnG0cmGZmBtQAzUDBv7l37z52fqXOzpjldSzt2ROd61u3Rq1i5cq4EiozgSBEGdva4l7dmkRQREpdMYNiPrAz53UDcEG/bb4A3A/sBqYB73D346bWM7NbgVsB6uuXAtGEs3VrPHp7Y7vMRH7798cEfqMp06TU2BjNTFOnwqpVx9citm+Pcm7dGn0ab3vb6JZTRGSoitlHkWf6Pbzf62uAjcCpwBrgC2Y2/bg3ud/p7mvdfe306XWvL+/pgepqOPXU7LZHjsT8S/1vTFRsnZ1xtdXu3fHZS5ceGxLd3RFoBw5E5/bBg3EjpMxEhCIipaqYQdEALMh5fRpRc8j1PuA+D1uAV4EVQ/2g/lc3VVTknyW2mGpr48+6upj+o3+ZzCLEmprgzDMjTHp74cc/ztaIRERKUTGbnp4ClpnZEmAXcCNwc79tdgBXA4+a2VzgTGBcXkBaXZ0dGZ7PtGmwenVMEVJbG81Thw/HXFTd3aN3Ka+IyFAVLSjcvcfMPgg8DJQDd7v7ZjO7LVl/B/BJ4B4ze45oqvqQuzcN5/Pa27P9FaNdmyjUvHnZ52ecEfNQNTSMXXlERApR1Lme3P0B4IF+y+7Ieb4beOtIfd6WLdEvMF4uQy3VQBMRyTWhRma7w+LFQ5sIcCy1tUUNaMeOsS6JiMjAxv3sseNZd3eExQ9/GPfMGM0xFb29MeZjz54Yi7Jy5fFTkIiIgIJiTM2bF/f0bmuDJ5+ESy+NPou2tph+pNAbLg3F0aNRg2lujkt09+yJ52Zw/vkj/3kiMv6N26Bob88+H6/zJ9XWwooVMavspk3xK7+8PF5PmwY33TRys992dsJrr8VYj8bGCIjycpg1K45l33HDHEVEwrgMivb2uDFRS0uc7Do6on/C+w/nGwcWLoxaRHt7jCjPnLRbW+GJJ+CSS06s07urKwKiuRn27YvPqqyEJUtizEdPj+6hISLpxmVndl9fDF47eBDmzImAOHgwmmoqxln0VVTAmjXZcFi1KmoTzc0xseBwB+N1dUWYrl8fU5yvXx83d1q2DH7pl2DmzAigTLju2hXH8LXXYiLDtOnQe3piX/v2jc9wFpGhGWen1WP1v092Mdr0R8P06XDVVXEL1aoqOOecGJh36NDQ99XVFX0QBw7EyXzv3qhBZDqr+9dO3KMW09AAP/hBbJMZBPie98RYj8x2hw9HkGRGmLe2Rrlzx4eIyMQzroNiIqmuTn9diL1744Tf1BRhUVkZA/umTx/4Pt6TJkU/ybPPRkBMnRqf3dERTWELFkTNYdeuCIiGhgiwysqogbS1Db2cIjK+KChKWF9f9MNUV8cJPMM92/ENceLesiV+8W/dGu9bvBhmzy7sc045JWoOLS0RDL/4RdQyfvKT2E9nZwREZ2fU2tasiean556LcKqujk7x4YSbiJQ+BUWJ6uyMk/GGDdEMdOmlUTNob4+rolpaopZQWxtB0dAQ/RoLFsSJf6hzR02fnh1HsWBB9FFkwsE9OyNv5iqs5uYo3/btEU7LlsHFF4/oIRCREqGgKFHd3dHf8MorUVPo6YlZZ1tb4yqlnTujVjFnTqyvqYlf+iPxq37qVLjmGti4MaZwnzPn+OCZMiWaoiZNimaqmTNP/HNFpDQpKEpUTU10HC9cGH0Bv/hF/Lpva4sO6crKCIiOjuhjmDp1ZOeOKi+HN75x4PXV1REmEGNAWlpiMN94mWdLRAo3IYKivj5OVBPJggXxgKhdbN0aNYnTT4+mJfeoVSxcOHZTlGc6yHt7o6bz/e/H6O45c47drq8v2/FdXR3lzTSbaWJEkdI3IYJi3ryJfYnmmWdGMNTWRkhkLFkydmXKNW1adKa3t0cg3HxzhEFnZ4TbwYMR5A0NERr19dGs9qY3Hft9RKQ0TYigOBmsGPJ9/0bP8uVRs3n88Ri/8dhjMcalvT0u1d29OwKipydqHi0tsW7q1KiB1NWpZiFSyhQUMiKqq6N2sHNnjMnIXKFVXR33D6+tjeamjo5oSnvsseiob2yECy+MWtN41NcXfUXl5VGzEpmIFBQyYlaujJPlpk0xruLMM4+f1HDy5HhceCE8/XTULurqYvu2tmz/RVlZdIx3d0ftZCzuMdLXF81nVVXxvLk5akhHj0b5+vrieUtL1JbWrIn7pat2JBONgkJG1GmnxWMwdXVw9dVxCe7Bg1EL2bMnaiEzZ8afNTXRUT5vXvRnjJTW1hgoaBafVVeXDany8giDpqao/bS2RiBUV8fzAweiBjFpUvQbdXdn7yvS0hJlnT07e6tbkYlAQSFjqqcnTrzt7fHLvbExTsCNjVE76e2NqURmzIgmrMbGOIn39UWNJW16kozMVVf798dntbbGIMWmpgiAFStiTEh3d5ShoyPWHTkSAeCenVesvj4+s7Ex+mZqamL9o4/GvUUaGyMg9u6NmX8rK4+vVbnHfpubo0ZSUQGLFg3+PUTGioJCxtSqVXESP/XU+IV/7rlxwuztjRPq5s1x8n3kkTixZqZLb22N+afq6+Ny3EWLjj0p9/TEto2NEUKHD2dv1uSeHSDY3Bwz63Z1xcm7ri462efOjSCoqorlU6Yc26SUuXQ544orIpAOHYomtS1bsttffHF8pwMH4vMy42EOHYpHeztce21c+ixSihQUMqamTDm2iSbzqzozNmT1anj++QiHzZujU3zhwpg6fffu+BU/ZUrULnp64pd/dXWcjDM3aTp8OE74s2bB2WfHerPY5tVX4zLj6uqoUZgdP1V97jxbacrKIoDmzo3ybtwYyw4fjlpIZ2eERaaZq64u+mv274ennoqJHM89N8JPpJSYj7MbCixduta/9KX1dHXFf+xFi8a6RFJsPT3RfzF/fjZIMs02Bw5EGJSXR62gpiYeHR1xwp83L07eoz1hYU9PBMMjj0T4zJoV5Zo9O0Ij0znf0wM//Wn8W66ujvC4/PKoXWRqHS0t8X1mzIj9FNLcJtKfmW1w97XDeu94DIqPfGQ9ZWVR/c9MZCcnr0OHspMjvvhiNEUtWDA2V0rl09cXYZF2NVRfX9QqWlvju6xZk20yO3Qolk+aFDWQ2bOj/2bRogjPgXR0xJ9lZdlBj2YRWKeeWljZe3vjUVkZQVxZOXBIucdnlpdrKpdSdNIFxV/91Xrmz1cVXSae556Lk3pVVZyUa2ujJtHZGeNOOjtjeXV1PN761vix1NsbJ/DW1giXzs5sB71ZrM/UTsrKoqN96tTsHF2Zy30hu+/W1thPZ+exZTz99KitZeYa6+yM92dugtXVFUExbVr0QeXeXEzGzokEhfooRErIypXRzHbKKcdfLZW57LivL8aqNDVF09aMGdGElam5HDwYr8vKYpu6ugiTmTPjxN7cHKPoJ0+O5q1M+GRuu1tVFUFw9Gi8zgwodI99bt8eIdDeHvvL1CIyNbjm5nheVRX9L1ddFcHT1RXbjbfbFYuCQqSkVFQcf0VVf2Vl0Smf6QCfMydqBlVV8d4zzohf8fkmi5w1K070DQ1xsl+2LELi0KEIkqamCKo5c6I2k+n47+uLx0svxfrq6mj2qqmJ93d2RvBkdHfH1WS7d8ODD0aYZZreLr00G4K9vfHeiooIErPYz0ATXbrH9pnLnMvK4nsUepMuGR4Fhcg4VFEBF100vPedcUY8cmVO8vPn5+/3yIyWP+usePSXGxIQQXDKKdFntH9/fG55ebbWM3NmhElHR7a5qqoq++fMmVGOzLiWtrbYrrMz29x15EgEXHU1vPOd2SBSR//IU1CISFEsWXLsDMf79sEzz8RlzlVV2aaujo5o5po1K65gq6yM5rLNmyOAKiuj9tDenr0XS+YKsfb2CIsnn4wQ6u7OdqYvXx61IjlxCgoRGRVz5sSlv5k+jxkz8vdXHDwYTWqNjdnmtFNPjc73/ldTZS6TfvrpeH3kSHbg5UsvwWWXZe/nbqbaxnApKERk1GSuqEpTVxePQqxcGWNlpk7N9nu4R/9IczP84Aexr/r6WD55coRTfX00bfW/YEDyU1CIyLjWv3nJLEb0b98ed4bcty87E3FtbfSDTJkSATNnTgRGbe3x/SySpaAQkQmnqir6KJYvj76LsrLoEO/piT6Rp5+OUf01NVEbqa+PpjD3uBKsvj5qG5oyPhQUFGb2JuBjwKLkPQa4uy8d5H3XAp8DyoGvuPun82xzBfBZoBJocvfLh1B+EZFUmX6JzJxdM2bAdddFR/jmzdEfsnNndnr7V1+NMSvl5VHbmDs322TlHmGTFiLu0W+SGTMyEcKm0BrFXcCfABuA3kLeYGblwO3AW4AG4Ckzu9/dX8jZphb4InCtu+8wszlDKbyIyHBNmQLnnRfPe3vjhL5+fVzO29QUy19+OQJk8uRovqqsjKCoqIjayMyZ8eeRI/E4ejQeHR3ZEeqZwYdnnTV+R6kXGhQt7v7gEPd9PrDF3bcBmNm9wPXACznb3Azc5+47ANx93xA/Q0TkhGUG+J1/fnZZU1PMALxvX/amVpMmRVCYRad8pmO8uzsu4c1MmdLVFbWTzPsqKmJa/HXrxmdfSKFB8WMz+3vgPuBoZqG7P53ynvnAzpzXDcAF/bZZDlSa2U+AacDn3P1rBZZJRKRoZs2CN785/7rM+I0XX4wT/8yZ0TmeCY9cPT3wxBMxov2hh2KwY6bzvKJifIz1KDQoMif43AmlHLgq5T35Wub6z0BYAbwRuBqYDDxuZk+4+8vH7MjsVuBWgPp63d1FRMZW5j4qhczCW1EBF1wAjz0W/SH79mVrIr29MR1LpjO9VC/XLSgo3P3KYey7AcidteY0YHeebZrcvQ1oM7NHgNXAMUHh7ncCd0LMHjuMsoiIjJnKyrgL4sGDcbXVzp3RTHXgQPRnVFdHE9U55xw/vUopKPSqpxnAR4HLkkU/BT7h7i0pb3sKWGZmS4BdwI1En0Su/wK+YGYVQBVRc/mnwosvIjJ+ZAYTnnFGXLb7859Hf8auXXG11P79cXVWqY3rKLTp6W7geeC3ktfvAr4K/PpAb3D3HjP7IPAwcXns3e6+2cxuS9bf4e4vmtlDwCagj7iE9vnhfRURkfGjrOzYiR2fey6unPre96IpasUKWLw4JkDs64sAGWhW3WIr6MZFZrbR3dcMtmw06MZFIjIRtbfH/eGbmqIPY8aMCIrKymimmjQp7nw43PPeaNy4qMPMLnH3/0k+8E1Ax3A+UEREjjdlSvby3AMHYkbcsrLov9izJ4KisRFu7t+APwoKDYrfB/4l6aswoBl4b7EKJSJyMquvjzEXGeecE2M6mpvjJlUzZoxueQqadNfdN7r7auBc4Bx3/yV3f7a4RRMRkYyqqhiT8YMfZO/lMVpSaxRm9k53/7qZ/Wm/5QC4+z8WsWwiIpJYvjyuknrttZjUMHN72lWril/DGKzpKZlGi2nFLYaIiKTJDNLbtSvmpHKPMRgHDsQI8srK4l0VlRoU7v7l5M+PF+fjRUSkUCtWxGy2U6bEnFJPPBGD9x54IKYRqa2NK6NGWkF9FGb2GTObbmaVZvZDM2sys3eOfHFERCRNXV12VtoLLoixF9u2wYYN8PjjcSntSCv0DrJvdffDwK8Q024sB/585IsjIiKFmjQJrroKrrwyahpdXTGnVF/fyH5OoUGRmarqOuCb7t48ssUQEZETMXdu1C42bowro7q6Rm7fhQbFd8zsF8TssT80s9lA58gVQ0RETsTMmXDxxXHL1+eeg4cfjrmjRkKh4yg+DFwErHX3bqCNuAmRiIiUiOnTY5bazs64V8b3vheX056owcZRXOXuPzKzX89ZlrvJfSdeBBERGSnV1XDttfDUU7B9e9z3e9GiE9vnYOMoLgd+BPxqnnWOgkJEpCS94Q3wzDMjs6/BxlF8NPnzfSPzcSIiMt4UOo7ib8ysNud1nZl9qnjFEhGRE+Eej61b4ZFHTmxfhV71tM7dD2UL4AeJS2VHnXtMjFVWaMlFRE5CFUl7UVNT9FfA9Jph76vA7crNbJK7HwUws8nApOF+6Ino64uRiaM9za6IyHhz3nnxZ9yfbvg/rwsNiq8T4ye+SnRi/zbwL8P90BNVXa0ahYhIoY69WHXoCgoKd/+MmW0C3kzcuOiT7v7wiX20iIiMB4XWKABeBHrc/QdmNsXMprl7a7EKJiIipaHQq55+F/gP4MvJovnAt4tVKBERKR2FtvR/AHgTcBjA3V8B5hSrUCIiUjoKDYqj7v76XIRmVkF0aouIyARXaFD81Mz+AphsZm8B/h34TvGKJSIipaLQoPgQsB94Dvg94AHgL4tVKBERKR2DXvVkZmXAJnc/G/jn4hdJRERKyaA1CnfvA541s4WjUCoDpYYAAAoLSURBVB4RESkxhY6jmAdsNrMniZsWAeDuv1aUUomISMkoNCg+XtRSiIhIyRrsDnfVwG3AGURH9l3u3jMaBRMRkdIwWB/FvwBriZBYB/xD0UskIiIlZbCmp1Xufg6Amd0FPFn8IomISCkZrEbRnXmiJicRkZPTYEGx2swOJ49W4NzMczM7PNjOzexaM3vJzLaY2YdTtjvPzHrN7IahfgERESmu1KYndy8f7o7NrBy4HXgL0AA8ZWb3u/sLebb7O0D3txARKUHFvE/c+cAWd9+WTCh4L3B9nu3+EPgWsK+IZRERkWEqZlDMB3bmvG5Ilr3OzOYDbwfuSNuRmd1qZuvNbP2RI4dGvKAiIjKwYgZFvru09p+a/LPAh9y9N21H7n6nu69197U1NbUjVkARERncUG6FOlQNwIKc16cBu/ttsxa41+LO37OA68ysx9119zwRkRJRzKB4ClhmZkuAXcCNwM25G7j7ksxzM7sH+K5CQkSktBQtKNy9x8w+SFzNVA7c7e6bzey2ZH1qv4SIiJSGYtYocPcHiJsc5S7LGxDu/t5ilkVERIanmJ3ZIiIyASgoREQklYJCRERSKShERCSVgkJERFIpKEREJJWCQkREUikoREQklYJCRERSKShERCSVgkJERFIpKEREJJWCQkREUikoREQklYJCRERSKShERCSVgkJERFIpKEREJJWCQkREUikoREQklYJCRERSKShERCSVgkJERFIpKEREJJWCQkREUikoREQklYJCRERSKShERCSVgkJERFIpKEREJJWCQkREUikoREQkVVGDwsyuNbOXzGyLmX04z/pbzGxT8viZma0uZnlERGToihYUZlYO3A6sA1YBN5nZqn6bvQpc7u7nAp8E7ixWeUREZHiKWaM4H9ji7tvcvQu4F7g+dwN3/5m7H0xePgGcVsTyiIjIMBQzKOYDO3NeNyTLBvJ+4MF8K8zsVjNbb2brjxw5NIJFFBGRwRQzKCzPMs+7odmVRFB8KN96d7/T3de6+9qamtoRLKKIiAymooj7bgAW5Lw+DdjdfyMzOxf4CrDO3Q8UsTwiIjIMxaxRPAUsM7MlZlYF3Ajcn7uBmS0E7gPe5e4vF7EsIiIyTEWrUbh7j5l9EHgYKAfudvfNZnZbsv4O4CNAPfBFMwPocfe1xSqTiIgMXTGbnnD3B4AH+i27I+f57wC/U8wyiIjIidHIbBERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVAoKERFJVdSgMLNrzewlM9tiZh/Os97M7PPJ+k1m9oZilkdERIauaEFhZuXA7cA6YBVwk5mt6rfZOmBZ8rgV+FKxyiMiIsNTUcR9nw9scfdtAGZ2L3A98ELONtcDX3N3B54ws1ozm+fue9J2fPQodHYWq9giIpKrmEExH9iZ87oBuKCAbeYDxwSFmd1K1DgA677hhtpt0Ns3wuUdh7rroPLgWJeiNOhYZOlYZOlYZLUvGu47ixkUlmeZD2Mb3P1O4E4AM1vvfmjtiRdv/Itj0aljgY5FLh2LLB2LLDNbP9z3FrMzuwFYkPP6NGD3MLYREZExVMygeApYZmZLzKwKuBG4v9829wPvTq5+uhBoGax/QkRERlfRmp7cvcfMPgg8DJQDd7v7ZjO7LVl/B/AAcB2wBWgH3lfAru8sUpHHIx2LLB2LLB2LLB2LrGEfC4sLjkRERPLTyGwREUmloBARkVQlGxSa/iOrgGNxS3IMNpnZz8xs9ViUczQMdixytjvPzHrN7IbRLN9oKuRYmNkVZrbRzDab2U9Hu4yjpYD/IzPM7Dtm9mxyLArpDx13zOxuM9tnZs8PsH545013L7kH0fm9FVgKVAHPAqv6bXMd8CAxFuNC4OdjXe4xPBYXA3XJ83Un87HI2e5HxMUSN4x1ucfw30UtMRPCwuT1nLEu9xgei78A/i55PhtoBqrGuuxFOBaXAW8Anh9g/bDOm6Vao3h9+g937wIy03/ken36D3d/Aqg1s3mjXdBRMOixcPefuXtm9OkTxHiUiaiQfxcAfwh8C9g3moUbZYUci5uB+9x9B4C7T9TjUcixcGCamRlQQwRFz+gWs/jc/RHiuw1kWOfNUg2Kgab2GOo2E8FQv+f7iV8ME9Ggx8LM5gNvB+4YxXKNhUL+XSwH6szsJ2a2wczePWqlG12FHIsvACuJAb3PAX/s7ifjNEDDOm8WcwqPEzFi039MAAV/TzO7kgiKS4paorFTyLH4LPAhd++NH48TViHHogJ4I3A1MBl43MyecPeXi124UVbIsbgG2AhcBZwOfN/MHnX3w8UuXIkZ1nmzVINC039kFfQ9zexc4CvAOnc/MEplG22FHIu1wL1JSMwCrjOzHnf/9ugUcdQU+n+kyd3bgDYzewRYDUy0oCjkWLwP+LRHQ/0WM3sVWAE8OTpFLBnDOm+WatOTpv/IGvRYmNlC4D7gXRPw12KuQY+Fuy9x98Xuvhj4D+APJmBIQGH/R/4LuNTMKsxsCjF784ujXM7RUMix2EHUrDCzucCZwLZRLWVpGNZ5syRrFF686T/GnQKPxUeAeuCLyS/pHnefcDNmFngsTgqFHAt3f9HMHgI2AX3AV9w972WT41mB/y4+CdxjZs8RzS8fcvemMSt0kZjZN4ErgFlm1gB8FKiEEztvagoPERFJVapNTyIiUiIUFCIikkpBISIiqRQUIiKSSkEhIiKpFBQieSQzz240s+eTWUdrR3j/281sVvL8yEjuW2SkKShE8utw9zXufjYxydoHxrpAImNFQSEyuMdJJk4zs9PN7KFkkr1HzWxFsnyumf1ncr+DZ83s4mT5t5NtN5vZrWP4HUSGrSRHZouUCjMrJ6Z+uCtZdCdwm7u/YmYXAF8kJpr7PPBTd3978p6aZPvfdvdmM5sMPGVm35rAc3HJBKWgEMlvspltBBYDG4jZRmuIm0T9e87MtJOSP68C3g3g7r1AS7L8j8zs7cnzBcAyQEEh44qCQiS/DndfY2YzgO8SfRT3AIfcfU0hOzCzK4A3Axe5e7uZ/QSoLk5xRYpHfRQiKdy9Bfgj4H8DHcCrZvab8Pr9hzP3J/8h8PvJ8nIzmw7MAA4mIbGCuPWkyLijoBAZhLs/Q9yH+UbgFuD9ZvYssJnsLTf/GLgymZ10A3AW8BBQYWabiNlLnxjtsouMBM0eKyIiqVSjEBGRVAoKERFJpaAQEZFUCgoREUmloBARkVQKChERSaWgEBGRVP8fOdz9OJU/YCAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at Precision of 80 is {} 0.15172413793103448\n",
      "  Accuracy: 19.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.58      0.58      1366\n",
      "           1       0.66      0.66      0.66      1697\n",
      "\n",
      "    accuracy                           0.62      3063\n",
      "   macro avg       0.62      0.62      0.62      3063\n",
      "weighted avg       0.62      0.62      0.62      3063\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-e84145ab84a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Readmitted vs not readmitted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Blues'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;31m# plt.savefig(f\"{output_dir}Confusion_Matrix.png\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from funcsigs import signature\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report\n",
    "from tqdm import tqdm_notebook\n",
    "# Prediction on test set\n",
    "output_dir = \"./results/results_updated_BioClinicalBERTpretrained_discharge_220421_v2/\"\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "load_pretrained = True\n",
    "if load_pretrained:\n",
    "    #reload other model perhaps: \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        './model/results_updated_BioClinicalBERTpretrained_discharge_220421/', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                        # You can increase this for multi-class tasks.   \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "else:\n",
    "    print(\"hopefully you ran another model earlier and are using that!\")\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "logits_history, pred_labels, pred_labels_history, true_labels_history = [], [], [], []\n",
    "\n",
    "\n",
    "# Tracking variables \n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "m = nn.Sigmoid()\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in tqdm(test_dataloader):\n",
    "\n",
    "    # Unpack this training batch from our dataloader. \n",
    "    #\n",
    "    # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "    # the `to` method.\n",
    "    #\n",
    "    # `batch` contains three pytorch tensors:\n",
    "    #   [0]: input ids \n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels \n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "\n",
    "\n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "#             loss, logits = model(b_input_ids, \n",
    "#                      token_type_ids=None, \n",
    "#                      attention_mask=b_input_mask, \n",
    "#                      labels=b_labels)\n",
    "\n",
    "    # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "    # output values prior to applying an activation function like the \n",
    "    # softmax.\n",
    "    loss = result.loss\n",
    "    logits = result.logits\n",
    "    \n",
    "#     print(\"original logits shape: \", logits.shape)\n",
    "    logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "    \n",
    "#     print(\"logit shape after sigmoid: \", logits.shape)\n",
    "#     break\n",
    "    \n",
    "\n",
    "    # Accumulate the evaluation loss.\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "#     logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    label_ids = b_labels.long().flatten().to('cpu').numpy()\n",
    "#     print(\"label_ids shape:\", label_ids.shape)   \n",
    "\n",
    "    \n",
    "      # Store predictions and true labels\n",
    "        \n",
    "\n",
    "    logits_history.append(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     logits_history = logits_history + logits.flatten().tolist()\n",
    "    pred_labels = np.argmax(logits, axis=1).flatten()\n",
    "#     print(\"pred labels shape: \", pred_labels.shape)\n",
    "    \n",
    "#     pred_labels_history.append(np.argmax(logits, axis=1).flatten())\n",
    "#     pred_labels = np.asarray([1 if i else 0 for i in (output_probs.flatten() >= 0.5)])\n",
    "    \n",
    "    pred_labels_history = pred_labels_history + pred_labels.tolist()\n",
    "    \n",
    "    true_labels_history.append(label_ids)\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences, and\n",
    "    # accumulate it over all batches.\n",
    "    \n",
    "    total_eval_accuracy += np.sum(pred_labels == label_ids)\n",
    "    \n",
    "#     total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "#     def flat_accuracy(preds, labels):\n",
    "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "#     labels_flat = labels.flatten()\n",
    "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "\n",
    "# concatenate a bunch of these     \n",
    "logits_history = np.concatenate(logits_history, axis=0)\n",
    "# pred_labels_history = np.concatenate(pred_labels_history, axis = 0 )\n",
    "true_labels_history = np.concatenate(true_labels_history, axis = 0)\n",
    "\n",
    "\n",
    "# put logits, predicted labels and true labels in a dataframe to use the vote_score and vote_pr_curve\n",
    "df = pd.DataFrame({'logits': logits_history[:,1], 'pred_label': pred_labels_history, 'label': true_labels_history})\n",
    "#         string = 'logits_clinicalbert_' + readmission_mode + '_chunks.csv'\n",
    "#         df.to_csv(os.path.join(output_dir, string))\n",
    "df.to_csv(f'{output_dir}logits_preds_truths.csv')\n",
    "\n",
    "df_test = pd.read_csv(\"./data/discharge/test.csv\")\n",
    "fpr, tpr, df_out = vote_score(df_test, logits_history[:,1], output_dir = output_dir)\n",
    "\n",
    "# string = 'logits_clinicalbert_' + readmission_mode + '_readmissions.csv'\n",
    "df_out.to_csv(f'{output_dir}logits_BioClinicalBert_discharge.csv')\n",
    "\n",
    "rp80 = vote_pr_curve(df_test, logits_history[:,1], output_dir=output_dir)\n",
    "\n",
    "# Report the final accuracy for this test run.\n",
    "avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
    "\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_test_loss = total_eval_loss / len(test_dataloader)\n",
    "\n",
    "# print(\"  Evaluation on test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "\n",
    "# preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(true_labels_history, pred_labels_history))\n",
    "\n",
    "#save classification report as a dataframe to csv\n",
    "pd.DataFrame(classification_report(true_labels_history,pred_labels_history, output_dict=True)).to_csv(f\"{output_dir}_classification_report.csv\")\n",
    "\n",
    "\n",
    "cf = confusion_matrix(true_labels_history, pred_labels_history, normalize = 'true')\n",
    "df_cf = pd.DataFrame(cf, ['not r/a', 'readmitted'], ['not r/a', 'readmitted'])\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.suptitle(\"Readmitted vs not readmitted\")\n",
    "sns.heatmap(df_cf, annot = True, cmap = 'Blues')\n",
    "# plt.savefig(f\"{output_dir}Confusion_Matrix.png\")\n",
    "\n",
    "# Record all statistics from this test\n",
    "\n",
    "test_result = {'eval_loss': avg_test_loss,\n",
    "          'eval_accuracy': avg_test_accuracy,          \n",
    "          'RP80': rp80}\n",
    "pd.DataFrame(test_result).to_csv(f\"{output_dir}_test_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5026947 , 0.5672617 , 0.47131103, 0.55332077, 0.77614754,\n",
       "       0.754901  , 0.7177073 , 0.65854925, 0.70566314, 0.72021365,\n",
       "       0.7459882 , 0.7188838 , 0.64941925, 0.7642852 , 0.7208453 ,\n",
       "       0.78422195, 0.7037815 , 0.3439405 , 0.763295  , 0.72006994,\n",
       "       0.72926307, 0.7600393 , 0.7573594 , 0.728904  , 0.6712802 ,\n",
       "       0.6647416 , 0.7551599 , 0.7140581 , 0.73986346, 0.7096463 ,\n",
       "       0.67387164, 0.72770035, 0.7444164 , 0.73684376, 0.732151  ,\n",
       "       0.5663622 , 0.5343498 , 0.5917802 , 0.53025395, 0.325066  ,\n",
       "       0.62181604, 0.4708202 , 0.30574903, 0.4490955 , 0.28432512,\n",
       "       0.60118973, 0.7708683 , 0.60505384, 0.68473285, 0.49717268,\n",
       "       0.68909734, 0.6279827 , 0.712208  , 0.6714269 , 0.56396943,\n",
       "       0.6648534 , 0.5826787 , 0.3685809 , 0.292271  , 0.33242878,\n",
       "       0.35729066, 0.4949058 , 0.6695508 , 0.27508712], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits_history = np.concatenate(logits_history, axis=0)\n",
    "logits_history[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(true_labels_history, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.3888888888888889,\n",
       "  'recall': 0.4666666666666667,\n",
       "  'f1-score': 0.42424242424242425,\n",
       "  'support': 15},\n",
       " '1': {'precision': 0.8260869565217391,\n",
       "  'recall': 0.7755102040816326,\n",
       "  'f1-score': 0.8,\n",
       "  'support': 49},\n",
       " 'accuracy': 0.703125,\n",
       " 'macro avg': {'precision': 0.607487922705314,\n",
       "  'recall': 0.6210884353741497,\n",
       "  'f1-score': 0.6121212121212122,\n",
       "  'support': 64},\n",
       " 'weighted avg': {'precision': 0.7236186594202899,\n",
       "  'recall': 0.703125,\n",
       "  'f1-score': 0.7119318181818183,\n",
       "  'support': 64}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(np.concatenate(true_labels_history, axis=0),pred_labels_history, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "108\n",
      "108\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1]),\n",
       " array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 1, 1]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 0, 1, 1, 0]),\n",
       " array([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 0, 0, 1, 0]),\n",
       " array([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0]),\n",
       " array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1]),\n",
       " array([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 1, 1]),\n",
       " array([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0]),\n",
       " array([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0]),\n",
       " array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 0, 0]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1]),\n",
       " array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 0, 0]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 0]),\n",
       " array([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0]),\n",
       " array([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 0, 1]),\n",
       " array([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0]),\n",
       " array([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0]),\n",
       " array([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 1, 1]),\n",
       " array([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0]),\n",
       " array([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0]),\n",
       " array([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 0, 0]),\n",
       " array([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 0, 0, 1, 1]),\n",
       " array([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " array([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 0, 1]),\n",
       " array([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 1, 0, 1, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 0, 1, 1]),\n",
       " array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1]),\n",
       " array([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1]),\n",
       " array([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 0, 0]),\n",
       " array([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1]),\n",
       " array([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1]),\n",
       " array([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0]),\n",
       " array([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1]),\n",
       " array([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1]),\n",
       " array([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1]),\n",
       " array([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1]),\n",
       " array([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 1, 1, 1, 1, 0]),\n",
       " array([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 0]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0]),\n",
       " array([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1]),\n",
       " array([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0]),\n",
       " array([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 0, 1]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 0, 0, 0]),\n",
       " array([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1]),\n",
       " array([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 1, 1]),\n",
       " array([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0]),\n",
       " array([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 1, 1, 1]),\n",
       " array([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0]),\n",
       " array([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 0, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0]),\n",
       " array([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 0, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 0])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d7cd83dd2e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59      1595\n",
      "           1       0.65      0.60      0.62      1861\n",
      "\n",
      "    accuracy                           0.61      3456\n",
      "   macro avg       0.61      0.61      0.61      3456\n",
      "weighted avg       0.61      0.61      0.61      3456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(np.concatenate(true_labels,axis=0), np.concatenate(pred_labels, axis=0)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5472"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5441, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5jscIM8R4Gv"
   },
   "source": [
    "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
    "\n",
    "We use MCC here because the classes are imbalanced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWcy0X1hirdx",
    "outputId": "ef5e6753-c244-406a-8141-5078d71b04ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 354 of 516 (68.60%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRaZQ4XC7kLs",
    "outputId": "d922af70-1216-4cfb-ac37-1dde75744fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/cdthome/xiz325/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(logits_history[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUM0UA1qJaVB"
   },
   "source": [
    "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n",
    "\n",
    "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "pyfY1tqxU0t9",
    "outputId": "5e477de2-e6a9-466a-9b36-f3651f2996df"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 2)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(logits_history, axis=0)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      7.57\n",
       "1      4.13\n",
       "2      6.93\n",
       "3      6.51\n",
       "4      5.52\n",
       "       ... \n",
       "379   -6.82\n",
       "380   -7.46\n",
       "381   -6.78\n",
       "382   -3.13\n",
       "383   -7.75\n",
       "Name: 1, Length: 384, dtype: float32"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.concatenate(logits_history, axis=0))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(pred_labels, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3456,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(true_labels, axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YrjAPX2V-l4"
   },
   "source": [
    "Now we'll combine the results for all of the batches and calculate our final MCC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCYZa1lQ8Jn8",
    "outputId": "b4650298-0e35-4ed8-be13-83f074a617ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MCC: 0.218\n"
     ]
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(logits_history, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3456,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(flat_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate roc etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXx0jPc4HUfZ"
   },
   "source": [
    "Cool! In about half an hour and without doing any hyperparameter tuning (adjusting the learning rate, epochs, batch size, ADAM properties, etc.) we are able to get a good score. \n",
    "\n",
    "> *Note: To maximize the score, we should remove the \"validation set\" (which we used to help determine how many epochs to train for) and train on the entire training set.*\n",
    "\n",
    "The library documents the expected accuracy for this benchmark [here](https://huggingface.co/transformers/examples.html#glue) as `49.23`.\n",
    "\n",
    "You can also look at the official leaderboard [here](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy). \n",
    "\n",
    "Note that (due to the small dataset size?) the accuracy can vary significantly between runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfjYoa6WmkN6"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlQG7qgkmf4n"
   },
   "source": [
    "This post demonstrates that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUmsUOIv8EUO"
   },
   "source": [
    "# Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2079Qyn8Mt8"
   },
   "source": [
    "## A1. Saving & Loading Fine-Tuned Model\n",
    "\n",
    "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ulTWaOr8QNY",
    "outputId": "1b73b37b-2598-4992-d6d7-0649f410b5c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./results_updated_BioClinicalBERT_3day_190421/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./results_updated_BioClinicalBERT_3day_190421/tokenizer_config.json',\n",
       " './results_updated_BioClinicalBERT_3day_190421/special_tokens_map.json',\n",
       " './results_updated_BioClinicalBERT_3day_190421/vocab.txt',\n",
       " './results_updated_BioClinicalBERT_3day_190421/added_tokens.json')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "\n",
    "# torch.save(model.state_dict(), output_dir + \"pytorch_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-tjHkR7lc1I"
   },
   "source": [
    "Let's check out the file sizes, out of curiosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqMzI3VTCZo5",
    "outputId": "96104fe5-67d0-4310-d778-58da5194c2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 428000K\n",
      "-rw-r--r-- 1 root root      1K Feb  2 17:10 config.json\n",
      "-rw-r--r-- 1 root root 427757K Feb  2 17:10 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root      1K Feb  2 17:10 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root      1K Feb  2 17:10 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root    227K Feb  2 17:10 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=K ./model_save/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr_bt2rFlgDn"
   },
   "source": [
    "The largest file is the model weights, at around 418 megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WUFUIQ8Cu8D",
    "outputId": "b0c9b6c6-5fb8-4d61-d28a-be4324be5a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 418M Feb  2 17:10 ./model_save/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=M ./model_save/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzGKvOFAll_e"
   },
   "source": [
    "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trr-A-POC18_"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to this Notebook instance.\n",
    "from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxlZsafTC-V5"
   },
   "outputs": [],
   "source": [
    "# Copy the model files to a directory in your Google Drive.\n",
    "!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0vstijw85SZ"
   },
   "source": [
    "The following functions will load the model back from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nskPzUM084zL"
   },
   "outputs": [],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = model_class.from_pretrained(output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIWouvDrGVAi"
   },
   "source": [
    "## A.2. Weight Decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f123ZAlF1OyW"
   },
   "source": [
    "The huggingface example includes the following code block for enabling weight decay, but the default decay rate is \"0.0\", so I moved this to the appendix.\n",
    "\n",
    "This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxSMw0FrptiL"
   },
   "outputs": [],
   "source": [
    "# This code is taken from:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
    "\n",
    "# Don't apply weight decay to any parameters whose names include these tokens.\n",
    "# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# Separate the `weight` parameters from the `bias` parameters. \n",
    "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
    "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
    "optimizer_grouped_parameters = [\n",
    "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.1},\n",
    "    \n",
    "    # Filter for parameters which *do* include those.\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# Note - `optimizer_grouped_parameters` only includes the parameter values, not \n",
    "# the names."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification v4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fe5b1d0540240a8a8426352c24b2887": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1058e0b5baa248faa60c1ad146d10bf7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1296a3d754b344a482a03e5af84e805e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8874fec8a404ae89a38fd2ecbb357cf",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2755b9838bae408ca8cf667ad9d501fc",
      "value": 433
     }
    },
    "1c2b0ede959142fc89bf07a9c88df638": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23ca9359e6c44232a1346e6f2ab7e48c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fe5b1d0540240a8a8426352c24b2887",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c7dec7b1e804c2195f6e60fb3c1d18e",
      "value": 440473133
     }
    },
    "2755b9838bae408ca8cf667ad9d501fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "375cc635389c4ddb9bf2aa443df58bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "440da34c72344cb08e4a1ee5de7049ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "472198d5b6a748b3a81f9364fd1fa711": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b1e27aff6f04fec8268d951e46b1e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c7dec7b1e804c2195f6e60fb3c1d18e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6f132d7bb83d41b6847df0d0ec0a1b92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_978c24b18b594eaf8ca47730a88eefb9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a7bdbedc75de4f77b45f1389c2ea0abc",
      "value": " 433/433 [00:00&lt;00:00, 2.02kB/s]"
     }
    },
    "82ddfcea0e4c4e5a86cf6eca8585be8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c76faadf2f4415393c6f0a805f0d72b",
       "IPY_MODEL_e0bb735fda99434a90380e7fc664212d"
      ],
      "layout": "IPY_MODEL_8a256ba4a19e4ec98fe3c3c99fba4daa"
     }
    },
    "8a256ba4a19e4ec98fe3c3c99fba4daa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c76faadf2f4415393c6f0a805f0d72b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1058e0b5baa248faa60c1ad146d10bf7",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdb78e75309f4bc09366533331e72431",
      "value": 231508
     }
    },
    "978c24b18b594eaf8ca47730a88eefb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7bdbedc75de4f77b45f1389c2ea0abc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf9dfa1ff3e642fbb74c5146d21044c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1296a3d754b344a482a03e5af84e805e",
       "IPY_MODEL_6f132d7bb83d41b6847df0d0ec0a1b92"
      ],
      "layout": "IPY_MODEL_1c2b0ede959142fc89bf07a9c88df638"
     }
    },
    "cdb78e75309f4bc09366533331e72431": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cea84f9c3db641acb98314028b305514": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d689bc8d488a4dc09c393b4fc9747bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_440da34c72344cb08e4a1ee5de7049ee",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4b1e27aff6f04fec8268d951e46b1e63",
      "value": " 440M/440M [00:07&lt;00:00, 55.5MB/s]"
     }
    },
    "e0bb735fda99434a90380e7fc664212d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_472198d5b6a748b3a81f9364fd1fa711",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_375cc635389c4ddb9bf2aa443df58bae",
      "value": " 232k/232k [00:00&lt;00:00, 616kB/s]"
     }
    },
    "f8874fec8a404ae89a38fd2ecbb357cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe254c3bcc08402eb506f0e98f5673a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23ca9359e6c44232a1346e6f2ab7e48c",
       "IPY_MODEL_d689bc8d488a4dc09c393b4fc9747bcb"
      ],
      "layout": "IPY_MODEL_cea84f9c3db641acb98314028b305514"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
